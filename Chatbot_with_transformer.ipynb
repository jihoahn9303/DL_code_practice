{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 라이브러리 설치\n",
        "- einops: 텐서 차원 조작을 도와주는 라이브러리\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_bZrjU-Rr_lT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bo7KeOM1rxsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ec02d8-0674-4b50-8299-66adabb9ead1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요 라이브러리 import\n"
      ],
      "metadata": {
        "id": "yH_lMmlZJdzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import urllib.request\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.framework.ops import EagerTensor\n",
        "from tensorflow.keras.layers import (\n",
        "    Layer,\n",
        "    LayerNormalization,\n",
        "    Dropout,\n",
        "    Embedding,\n",
        "    Softmax\n",
        ")\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, Loss\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy, Metric\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.math import rsqrt, minimum, argmax\n",
        "\n",
        "from einops import rearrange, pack, unpack, einsum\n",
        "from einops.layers.tensorflow import EinMix"
      ],
      "metadata": {
        "id": "pX4ITz14J10k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 챗봇 데이터 로드\n",
        "- 출처: https://github.com/songys/Chatbot_data\n",
        "  - 챗봇 트레이닝용 문답 페어 11,876개\n",
        "  - 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링"
      ],
      "metadata": {
        "id": "F4t3RI2DJKcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n",
        "\n",
        "train_data = pd.read_csv('ChatBotData.csv')\n",
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xeEHpBSTJGIe",
        "outputId": "d9739dfa-a5ab-468b-d689-20178bec8795"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-04c491c9-300d-4a29-a749-e6a690607790\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04c491c9-300d-4a29-a749-e6a690607790')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-04c491c9-300d-4a29-a749-e6a690607790 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-04c491c9-300d-4a29-a749-e6a690607790');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c18fa36e-d921-4e28-af00-88f1bd1d4210\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c18fa36e-d921-4e28-af00-88f1bd1d4210')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c18fa36e-d921-4e28-af00-88f1bd1d4210 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBcQLhpHKM8H",
        "outputId": "c00eb852-2d19-435c-80e3-e625965f5ba3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q        0\n",
            "A        0\n",
            "label    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCIg-xanKrzO",
        "outputId": "4f2bb1e4-1f71-4ab1-e6bc-1cbd5a262556"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    5290\n",
              "1    3570\n",
              "2    2963\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리 함수 구현"
      ],
      "metadata": {
        "id": "3A9RU7iLJ7xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    # 구두점에 대해서 띄어쓰기\n",
        "    # ex) 12시 땡! -> 12시 땡 !\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "rxg6jmpvLVTn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대 길이를 40으로 정의\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    tokenized_inputs.append(sentence1)\n",
        "    tokenized_outputs.append(sentence2)\n",
        "\n",
        "  # 패딩\n",
        "  tokenized_inputs = pad_sequences(\n",
        "      tokenized_inputs,\n",
        "      maxlen=MAX_LENGTH,\n",
        "      padding='post'\n",
        "  )\n",
        "  tokenized_outputs = pad_sequences(\n",
        "      tokenized_outputs,\n",
        "      maxlen=MAX_LENGTH,\n",
        "      padding='post'\n",
        "  )\n",
        "\n",
        "  return tokenized_inputs, tokenized_outputs"
      ],
      "metadata": {
        "id": "yzVI4Y18KL0n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리 수행\n"
      ],
      "metadata": {
        "id": "HJAJeYRHLdB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 질의응답 문장 전처리(1)\n",
        "- 구두점 제거"
      ],
      "metadata": {
        "id": "R6PH7rtTMHBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [preprocess_sentence(sentence) for sentence in train_data['Q']]\n",
        "answers = [preprocess_sentence(sentence) for sentence in train_data['A']]"
      ],
      "metadata": {
        "id": "7pJNil49LvZb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVxBYFIhMcl3",
        "outputId": "acb05c3b-d80e-48b3-c4f8-923d68de1a6e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['12시 땡 !', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBJTXfNSMeLm",
        "outputId": "3fb7329d-6dc6-4147-88ae-ed7275a340cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['하루가 또 가네요 .', '위로해 드립니다 .', '여행은 언제나 좋죠 .', '여행은 언제나 좋죠 .', '눈살이 찌푸려지죠 .']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단어집합 생성"
      ],
      "metadata": {
        "id": "kc95VhATMmYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SubwordTextEncoder를 사용하여 질문과 답변을 모두 포함한 단어 집합(Vocabulary) 생성\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    questions + answers, target_vocab_size=2**13)\n",
        "\n",
        "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 2만큼 증가\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2"
      ],
      "metadata": {
        "id": "k78hnNLsMfae"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('시작 토큰 번호 :',START_TOKEN)\n",
        "print('종료 토큰 번호 :',END_TOKEN)\n",
        "print('단어 집합의 크기 :',VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mF8mLa0M7vA",
        "outputId": "afe675d6-3bcb-42a3-eca8-bffd5c48cad2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "시작 토큰 번호 : [8178]\n",
            "종료 토큰 번호 : [8179]\n",
            "단어 집합의 크기 : 8180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(questions[20])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6So-nvvNHpX",
        "outputId": "8963c5eb-28c6-41ac-de3f-c625af156a9a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sample question: [5766, 611, 3509, 141, 685, 3747, 849]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의의 입력 문장을 sample_string에 저장\n",
        "sample_string = questions[10]\n",
        "\n",
        "# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh6421SYNQ_X",
        "outputId": "7750baf1-3931-454c-e39a-cbae58f69a45"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정수 인코딩 후의 문장 [5779, 484, 194, 2984, 45, 3818]\n",
            "기존 문장: SNS보면 나만 빼고 다 행복해보여\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 정수는 각 단어와 어떻게 mapping되는지 병렬로 출력\n",
        "# SubwordTextEncoder는 의미있는 단위의 서브워드로 토크나이징한다.\n",
        "# 띄어쓰기 단위 X,  형태소 분석 단위 X\n",
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXhfMf88NWhg",
        "outputId": "b9903b9d-753c-4111-f9b4-254b21cbca98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5779 ----> SNS\n",
            "484 ----> 보면 \n",
            "194 ----> 나만 \n",
            "2984 ----> 빼고 \n",
            "45 ----> 다 \n",
            "3818 ----> 행복해보여\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 질의응답 문장 전처리(2)\n",
        "- 토큰화\n",
        "- 정수 인코딩\n",
        "- 문장 시작 및 종료 토큰 추가\n",
        "- 패딩"
      ],
      "metadata": {
        "id": "-EWOAAQONoZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions, answers = tokenize_and_filter(questions, answers)"
      ],
      "metadata": {
        "id": "v3O87uysNbD3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('질문 데이터의 크기(shape) :', questions.shape)\n",
        "print('답변 데이터의 크기(shape) :', answers.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV5fFIraORZf",
        "outputId": "da7eccf6-a037-4219-d379-367b91d7b3f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문 데이터의 크기(shape) : (11823, 40)\n",
            "답변 데이터의 크기(shape) : (11823, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7번째 샘플을 임의로 출력\n",
        "print(questions[6])\n",
        "print(answers[6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EryzrvROTZf",
        "outputId": "dbfd122b-8c89-46ec-ff8a-9a151589d503"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8178 8005 7990 2192  199 8179    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "[8178   69 2064  456    5  137 2188   17    1 8179    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('단어 집합의 크기(Vocab size): {}'.format(VOCAB_SIZE))\n",
        "print('전체 샘플의 수(Number of samples): {}'.format(len(questions)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6fVc9zEOWxH",
        "outputId": "d2d96088-b97d-4162-efe9-36dc892c9229"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합의 크기(Vocab size): 8180\n",
            "전체 샘플의 수(Number of samples): 11823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 생성\n",
        "- 올바른 번역 학습을 위하여 Teacher forcing 방법을 사용\n",
        "- 따라서, 라벨을 따로 분리하여 저장"
      ],
      "metadata": {
        "id": "aa4DIE1YOcCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n",
        "# 또한, 이 과정에서 Teacher forcing을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions[:, :-1],\n",
        "        'dec_inputs': answers[:, :-1], # 디코더의 입력. 마지막 패딩 토큰이 제거된다.,\n",
        "        'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n",
        "    }\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()    # 같은 데이터를 여러번 가져올 경우, 시간 감축을 위하여 캐싱 사용\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)   # 데이터 셔플링\n",
        "dataset = dataset.batch(BATCH_SIZE)      # 배치 사이즈만큼, 데이터 적재\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)   # 모델 학습 시, 백그라운드 스레드를 이용하여 다음 step에 사용할 데이터 미리 적재"
      ],
      "metadata": {
        "id": "-ECvjYYwOY7f"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataset:\n",
        "    print(data)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtsZxtK-y8Qd",
        "outputId": "bd7d2ffb-a718-4fb7-b931-6b722a4a6542"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'inputs': <tf.Tensor: shape=(64, 39), dtype=int32, numpy=\n",
            "array([[8178,   28,   76, ...,    0,    0,    0],\n",
            "       [8178,  715,  224, ...,    0,    0,    0],\n",
            "       [8178, 1095,   80, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [8178, 3433,  207, ...,    0,    0,    0],\n",
            "       [8178, 8156, 8102, ...,    0,    0,    0],\n",
            "       [8178, 7002, 3121, ...,    0,    0,    0]], dtype=int32)>, 'dec_inputs': <tf.Tensor: shape=(64, 39), dtype=int32, numpy=\n",
            "array([[8178, 1211,    1, ...,    0,    0,    0],\n",
            "       [8178, 1681, 1475, ...,    0,    0,    0],\n",
            "       [8178, 1255, 2941, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [8178, 3497,   93, ...,    0,    0,    0],\n",
            "       [8178, 2976, 3298, ...,    0,    0,    0],\n",
            "       [8178, 4161,  360, ...,    0,    0,    0]], dtype=int32)>, 'outputs': <tf.Tensor: shape=(64, 39), dtype=int32, numpy=\n",
            "array([[1211,    1, 8179, ...,    0,    0,    0],\n",
            "       [1681, 1475, 1768, ...,    0,    0,    0],\n",
            "       [1255, 2941,   10, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [3497,   93, 3785, ...,    0,    0,    0],\n",
            "       [2976, 3298, 7954, ...,    0,    0,    0],\n",
            "       [4161,  360,    1, ...,    0,    0,    0]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer 구현\n",
        "- Positional Encoding\n",
        "- Multi-Head attention\n",
        "- Encoder layer\n",
        "- Encoder\n",
        "- Decoder layer\n",
        "- Decoder\n",
        "- Transformer"
      ],
      "metadata": {
        "id": "7Mu0DGgfQ7tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "ayCygAx4RgOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      position: int,\n",
        "      embed_dim: int\n",
        "  ):\n",
        "      super(PositionalEncoding, self).__init__()\n",
        "      self.pos_encoding = self.positional_encoding(position, embed_dim)\n",
        "\n",
        "  def get_angles(\n",
        "      self,\n",
        "      position,\n",
        "      idx,\n",
        "      embed_dim: int\n",
        "    ) -> EagerTensor:\n",
        "      angles = 1 / tf.pow(10000, (2 * (idx // 2)) / tf.cast(embed_dim, tf.float32))\n",
        "      return position * angles   # (position, embed_dim)\n",
        "\n",
        "  def positional_encoding(\n",
        "      self,\n",
        "      position: int,\n",
        "      embed_dim: int\n",
        "    ) -> EagerTensor:\n",
        "      angle_rads = self.get_angles(\n",
        "          position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "          idx=tf.range(embed_dim, dtype=tf.float32)[tf.newaxis, :],\n",
        "          embed_dim=embed_dim\n",
        "      )  # (position=sequence_len, embed_dim)\n",
        "\n",
        "      # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
        "      sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "\n",
        "      # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
        "      cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "      angle_rads = np.zeros(angle_rads.shape)\n",
        "      angle_rads[:, 0::2] = sines\n",
        "      angle_rads[:, 1::2] = cosines\n",
        "      pos_encoding = tf.constant(angle_rads)   # (sequence_len, embed_dim)\n",
        "      pos_encoding = pos_encoding[tf.newaxis, ...]  # (1, sequence_len, embed_dim)\n",
        "\n",
        "      return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]   # (batch_size, sequence_len, embed_dim)"
      ],
      "metadata": {
        "id": "ofm3U5ZmPzfX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_pos_encoding = PositionalEncoding(50, 128)\n",
        "\n",
        "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 128))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "pSwy5QD7U-tR",
        "outputId": "9dd4dde4-e8a4-48b7-917d-205d39440423"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(sample_pos_encoding.pos_encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQe_vqSPWsB8",
        "outputId": "df792fd8-f3ca-4ece-da40-c3e8f3ced97b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention"
      ],
      "metadata": {
        "id": "qC9Zy-Q3Vc6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 마스크를 구현하는 방법은 입력된 정수 시퀀스에서 패딩 토큰의 인덱스인지, 아닌지를 판별하는 함수를 구현하는 것입니다.\n",
        "# 아래의 함수는 정수 시퀀스에서 0인 경우에는 1로 변환하고, 그렇지 않은 경우에는 0으로 변환하는 함수입니다.\n",
        "def create_padding_mask(tensor):\n",
        "    '''\n",
        "    tensor: (batch_size, sequence_len)\n",
        "    '''\n",
        "    mask = tf.cast(tf.math.equal(tensor, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, key의 문장 길이)"
      ],
      "metadata": {
        "id": "T0TTCG6fdKyN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionLayer(Layer):\n",
        "  def __init__(\n",
        "      self,\n",
        "      embed_dim: int = 512,   # 임베딩 차원\n",
        "      head_num: int = 8,      # 헤드 갯수\n",
        "  ):\n",
        "    super(MultiHeadAttentionLayer, self).__init__()\n",
        "    self.head_num = head_num\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    assert self.embed_dim % self.head_num == 0\n",
        "\n",
        "    self.mh_dim = self.embed_dim // self.head_num  # 멀티 헤드 어텐션 내 임베딩 차원\n",
        "\n",
        "    # Dense layer와 기능이 같지만, tensor 입출력 구조를 손쉽게 확인 가능함\n",
        "    self.to_query = EinMix(\n",
        "      pattern='batch sequence dim_in -> batch sequence dim_out',\n",
        "      weight_shape='dim_in dim_out',\n",
        "      bias_shape='dim_out',\n",
        "      dim_in=embed_dim,\n",
        "      dim_out=embed_dim\n",
        "    )\n",
        "\n",
        "    self.to_key = EinMix(\n",
        "      pattern='batch sequence dim_in -> batch sequence dim_out',\n",
        "      weight_shape='dim_in dim_out',\n",
        "      bias_shape='dim_out',\n",
        "      dim_in=embed_dim,\n",
        "      dim_out=embed_dim\n",
        "    )\n",
        "\n",
        "    self.to_value = EinMix(\n",
        "      pattern='batch sequence dim_in -> batch sequence dim_out',\n",
        "      weight_shape='dim_in dim_out',\n",
        "      bias_shape='dim_out',\n",
        "      dim_in=embed_dim,\n",
        "      dim_out=embed_dim\n",
        "    )\n",
        "\n",
        "    # 멀티 헤드 어텐션 출력층에 대응하는 레이어\n",
        "    self.dense = EinMix(\n",
        "        pattern='batch sequence dim_in -> batch sequence dim_out',\n",
        "        weight_shape='dim_in dim_out',\n",
        "        bias_shape='dim_out',\n",
        "        dim_in=embed_dim,\n",
        "        dim_out=embed_dim\n",
        "    )\n",
        "\n",
        "  def call(\n",
        "      self,\n",
        "      query,\n",
        "      key,\n",
        "      value,\n",
        "      mask\n",
        "    ):\n",
        "    '''\n",
        "    query, key, value: (batch_size, sequence_len, embed_dim)\n",
        "    mask: (batch_size, 1, 1, sequence_len)\n",
        "    '''\n",
        "    # 1. query, key, value 텐서 얻기\n",
        "    query = self.to_query(query)\n",
        "    key = self.to_key(key)\n",
        "    value = self.to_value(value)\n",
        "\n",
        "    # 2. query, key, value에 대해 멀티 헤드 텐서로 변경\n",
        "    query, key, value = map(\n",
        "        lambda x: rearrange(tensor=x, pattern='b s (h d) -> b h s d', h=self.head_num),\n",
        "        (query, key, value)\n",
        "    )  # (batch_size, multi-head_num, sequence_len, multi-head_embed_dim)\n",
        "\n",
        "    # 3. dot product (query-key)\n",
        "    energy = einsum(\n",
        "        query,\n",
        "        value,\n",
        "        'b h q d, b h k d -> b h q k'\n",
        "    )  # (batch_size, multi-head_num, sequence_len, sequence_len)\n",
        "\n",
        "    # 4. scaled dot product\n",
        "    scaled_energy = energy * self.mh_dim**-0.5   # (batch_size, multi-head_num, sequence_len, sequence_len)\n",
        "    if mask is not None:  # mask는 패딩 마스크 또는 look-ahead 마스크 둘 중 하나\n",
        "        scaled_energy += (mask * -1e9)\n",
        "\n",
        "    # 5. attention score 계산\n",
        "    attention_score = tf.nn.softmax(scaled_energy, axis=-1)  # (batch size, number of multi-head, sequence_len, sequence_len)\n",
        "\n",
        "    # 6. attention vector 계산\n",
        "    outputs = einsum(\n",
        "        attention_score,\n",
        "        value,\n",
        "        'b h q k, b h k d -> b h q d'\n",
        "    )  # (batch size, number of multi-head, sequence length, multi-head embed_dim)\n",
        "    outputs = rearrange(\n",
        "        outputs,\n",
        "        pattern='b h q d -> b q (h d)'\n",
        "    )  # (batch size, sequence length, embed_dim)\n",
        "\n",
        "    # 7. Dense layer 통과 (for Residual connection)\n",
        "    outputs = self.dense(outputs)\n",
        "\n",
        "\n",
        "    return outputs  # (batch size, sequence length, embed_dim)"
      ],
      "metadata": {
        "id": "HKbiuPMfVB5x"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed-forward"
      ],
      "metadata": {
        "id": "cWYMfbPLkld1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardLayer(Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int,\n",
        "        embed_dim: int,\n",
        "    ):\n",
        "        super(FeedForwardLayer, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.dense_1 = EinMix(\n",
        "            pattern=\"batch sequence dim_in -> batch sequence dim_out\",\n",
        "            weight_shape=\"dim_in dim_out\",\n",
        "            bias_shape=\"dim_out\",\n",
        "            dim_in=embed_dim,\n",
        "            dim_out=hidden_dim\n",
        "        )\n",
        "        self.dense_2 = EinMix(\n",
        "            pattern=\"batch sequence dim_out -> batch sequence dim_in\",\n",
        "            weight_shape=\"dim_out dim_in\",\n",
        "            bias_shape=\"dim_in\",\n",
        "            dim_in=embed_dim,\n",
        "            dim_out=hidden_dim\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs\n",
        "    ):\n",
        "        outputs = self.dense_1(inputs)\n",
        "        outputs = self.dense_2(outputs)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "UE3hR1gYkjRt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Layer"
      ],
      "metadata": {
        "id": "DrZzEP8rl469"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int = 512,       # 임베딩 차원\n",
        "        head_num: int = 8,          # 헤드 갯수\n",
        "        hidden_dim: int = 2048,     # 히든 레이어 차원\n",
        "        dropout_ratio: float = 0.3, # 드롭아웃 비율\n",
        "    ):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.ff_layer_norm = LayerNormalization(axis=-1)\n",
        "        self.mh_layer_norm = LayerNormalization(axis=-1)\n",
        "        self.mh_attention = MultiHeadAttentionLayer(\n",
        "            embed_dim=embed_dim,\n",
        "            head_num=head_num\n",
        "        )\n",
        "        self.ff = FeedForwardLayer(\n",
        "            hidden_dim=hidden_dim,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "        self.dropout = Dropout(rate=dropout_ratio)\n",
        "\n",
        "    def call(self, inputs, pad_mask):\n",
        "        '''\n",
        "        inputs: (batch_size, sequence_len, embed_dim)\n",
        "        pad_mask: None or (batch_size, 1, 1, sequence_len)\n",
        "        '''\n",
        "        # multi-head attention\n",
        "        mh_outputs = self.mh_attention(\n",
        "            query=inputs,\n",
        "            key=inputs,\n",
        "            value=inputs,\n",
        "            mask=pad_mask\n",
        "        )\n",
        "        # (batch_size, sequence_len, embed_dim)\n",
        "        outputs = self.mh_layer_norm(self.dropout(mh_outputs) + inputs)\n",
        "\n",
        "        # feed-forward propagation\n",
        "        ff_outputs = self.ff(outputs)\n",
        "        outputs = self.ff_layer_norm(self.dropout(ff_outputs) + outputs)\n",
        "\n",
        "        return outputs    # (batch_size, sequence_len, embed_dim)"
      ],
      "metadata": {
        "id": "71Nbw5K5lp7J"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder"
      ],
      "metadata": {
        "id": "DzHN-xM7rzvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        sequence_len: int,\n",
        "        layer_num: int,\n",
        "        embed_dim: int,\n",
        "        head_num: int = 8,          # 헤드 갯수\n",
        "        hidden_dim: int = 2048,     # 히든 레이어 차원\n",
        "        dropout_ratio: float = 0.3, # 드롭아웃 비율\n",
        "    ):\n",
        "        super(Encoder, self).__init__(name='Encoder')\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(\n",
        "            position=sequence_len,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "        self.encoder_list = [\n",
        "            EncoderLayer(embed_dim, head_num, hidden_dim, dropout_ratio)\n",
        "            for _ in range(layer_num)\n",
        "        ]\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        '''\n",
        "        inputs: (batch_size, sequence_len)\n",
        "        '''\n",
        "        # positional encoding\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.cast(self.embed_dim, tf.float32) ** -0.5\n",
        "        outputs = self.pos_encoding(outputs)   # (batch_size, sequence_len, embed_dim)\n",
        "\n",
        "        # generate padding mask\n",
        "        pad_mask = create_padding_mask(tensor=inputs)    # (batch_size, 1, 1, sequence_len)\n",
        "\n",
        "        # encoder layer\n",
        "        for encoder in self.encoder_list:\n",
        "            outputs = encoder(inputs=outputs, pad_mask=pad_mask)\n",
        "\n",
        "        return outputs   # (batch_size, sequence_len, embed_dim)"
      ],
      "metadata": {
        "id": "wc3t8vbhr51D"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Layer"
      ],
      "metadata": {
        "id": "mUSz3OV2-Cr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n",
        "def create_look_ahead_mask(tensor):\n",
        "    '''\n",
        "    tensor: (batch_size, sequence_len)\n",
        "    '''\n",
        "    seq_len = tf.shape(tensor)[-1]\n",
        "    # (sequence_len, sequence_len)\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(tensor)   # (batch_size, 1, 1, sequence_len)\n",
        "\n",
        "    # 패딩 마스크와 룩어헤드 마스크를 함께 고려\n",
        "    # (batch_size, 1, sequence_len, sequence_len)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "KYhnHTZu-EGt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int = 512,\n",
        "        head_num: int = 8,\n",
        "        hidden_dim: int = 2048,\n",
        "        dropout_ratio: float = 0.3, # 드롭아웃 비율\n",
        "    ):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # masked multi-head attention layer norm\n",
        "        self.mmh_layer_norm = LayerNormalization(axis=-1)\n",
        "        # multi-head attention layer norm\n",
        "        self.mh_layer_norm = LayerNormalization(axis=-1)\n",
        "        # feed forward layer norm\n",
        "        self.ff_layer_norm = LayerNormalization(axis=-1)\n",
        "\n",
        "        # masked multi-head attention\n",
        "        self.mmh_attention = MultiHeadAttentionLayer(\n",
        "            embed_dim=embed_dim,\n",
        "            head_num=head_num\n",
        "        )\n",
        "        # multi-head attention (Encoder-Decoder attention)\n",
        "        self.mh_attention = MultiHeadAttentionLayer(\n",
        "            embed_dim=embed_dim,\n",
        "            head_num=head_num\n",
        "        )\n",
        "        # feed forward\n",
        "        self.ff = FeedForwardLayer(\n",
        "            hidden_dim=hidden_dim,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "        self.dropout = Dropout(rate=dropout_ratio)\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs,\n",
        "        enc_output,\n",
        "        look_ahead_mask,\n",
        "        padding_mask\n",
        "    ):\n",
        "        '''\n",
        "        inputs: (batch_size, sequence_len, embed_dim)\n",
        "        enc_output: (batch_size, sequence_len, embed_dim)\n",
        "        look_ahead_mask: (batch_size, 1, sequence_len, sequence_len)\n",
        "        padding_mask: (batch_size, 1, 1, sequence_len)\n",
        "        '''\n",
        "        # masked multi-head attention\n",
        "        mmh_outputs = self.mmh_attention(\n",
        "            query=inputs,\n",
        "            key=inputs,\n",
        "            value=inputs,\n",
        "            mask=look_ahead_mask\n",
        "        )\n",
        "        # (batch size, sequence length, embed_dim)\n",
        "        mmh_outputs = self.mmh_layer_norm(self.dropout(mmh_outputs) + inputs)\n",
        "\n",
        "        # multi-head attention\n",
        "        mh_outputs = self.mh_attention(\n",
        "            query=enc_output,\n",
        "            key=enc_output,\n",
        "            value=mmh_outputs,\n",
        "            mask=padding_mask\n",
        "        )\n",
        "        mh_outputs = self.mh_layer_norm(self.dropout(mh_outputs) + mmh_outputs)\n",
        "\n",
        "        # feed forward network propagation\n",
        "        outputs = self.ff(mh_outputs)\n",
        "        outputs = self.ff_layer_norm(self.dropout(outputs) + mh_outputs)\n",
        "\n",
        "        return outputs  # (batch size, sequence length, embed_dim)\n"
      ],
      "metadata": {
        "id": "2Dz-XXaDHjNS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder"
      ],
      "metadata": {
        "id": "Qze6pxbxRPAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        sequence_len: int,\n",
        "        layer_num: int,\n",
        "        embed_dim: int,\n",
        "        head_num: int = 8,          # 헤드 갯수\n",
        "        hidden_dim: int = 2048,     # 히든 레이어 차원\n",
        "        dropout_ratio: float = 0.3, # 드롭아웃 비율\n",
        "    ):\n",
        "        super(Decoder, self).__init__(name='Decoder')\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(\n",
        "            position=sequence_len,\n",
        "            embed_dim=embed_dim\n",
        "        )\n",
        "        self.decoder_list = [\n",
        "            DecoderLayer(embed_dim, head_num, hidden_dim, dropout_ratio)\n",
        "            for _ in range(layer_num)\n",
        "        ]\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs,\n",
        "        enc_output\n",
        "    ):\n",
        "        '''\n",
        "        inputs: (batch_size, sequence_len)\n",
        "        enc_output: (batch_size, sequence_len, embed_dim)\n",
        "        '''\n",
        "        # position embedding\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.cast(self.embed_dim, tf.float32) ** -0.5\n",
        "        outputs = self.pos_encoding(outputs)   # (batch_size, sequence_len, embed_dim)\n",
        "\n",
        "        # generate look-ahead mask\n",
        "        look_ahead_mask = create_look_ahead_mask(inputs)\n",
        "\n",
        "        # generate padding mask\n",
        "        padding_mask = create_padding_mask(inputs)\n",
        "\n",
        "        # decoder layer\n",
        "        for decoder in self.decoder_list:\n",
        "            outputs = decoder(\n",
        "                inputs=outputs,\n",
        "                enc_output=enc_output,\n",
        "                look_ahead_mask=look_ahead_mask,\n",
        "                padding_mask=padding_mask\n",
        "            )\n",
        "\n",
        "        return outputs   # (batch_size, sequence_len, embed_dim)\n"
      ],
      "metadata": {
        "id": "UD2dP9L5RLHW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "EAbf4niAUcq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        sequence_len: int,\n",
        "        layer_num: int,\n",
        "        embed_dim: int,\n",
        "        head_num: int = 8,          # 헤드 갯수\n",
        "        hidden_dim: int = 2048,     # 히든 레이어 차원\n",
        "        dropout_ratio: float = 0.3, # 드롭아웃 비율\n",
        "\n",
        "    ):\n",
        "        super(Transformer, self).__init__(name='Transformer')\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size,\n",
        "            sequence_len,\n",
        "            layer_num,\n",
        "            embed_dim,\n",
        "            head_num,\n",
        "            hidden_dim,\n",
        "            dropout_ratio,\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size,\n",
        "            sequence_len,\n",
        "            layer_num,\n",
        "            embed_dim,\n",
        "            head_num,\n",
        "            hidden_dim,\n",
        "            dropout_ratio,\n",
        "        )\n",
        "        self.dense = EinMix(\n",
        "            pattern='batch sequence dim_in -> batch sequence dim_out',\n",
        "            weight_shape='dim_in dim_out',\n",
        "            bias_shape='dim_out',\n",
        "            dim_in=embed_dim,\n",
        "            dim_out=vocab_size\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        '''\n",
        "        inputs: [(batch_size, sequence_len), (batch_size, sequence_len)]\n",
        "        '''\n",
        "        enc_output = self.encoder(inputs[0])\n",
        "        # (batch_size, sequence_len, embed_dim)\n",
        "        dec_output = self.decoder(inputs[1], enc_output)\n",
        "        # (batch_size, sequence_len, vocab_size)\n",
        "        outputs = self.dense(dec_output)\n",
        "        outputs = Softmax(axis=-1)(outputs)\n",
        "\n",
        "        return outputs  # (batch_size, sequence_len, vocab_size)"
      ],
      "metadata": {
        "id": "jzgJkOrhUeJ6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "SYwW8bX15_At"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 생성"
      ],
      "metadata": {
        "id": "XeieftoD6Eke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    sequence_len=MAX_LENGTH,\n",
        "    layer_num=3,\n",
        "    embed_dim=512,\n",
        "    head_num=8,\n",
        "    hidden_dim=512,\n",
        "    dropout_ratio=0.3\n",
        ")"
      ],
      "metadata": {
        "id": "IsvGUtluXqc2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer 학습"
      ],
      "metadata": {
        "id": "PAceHNDb6GhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 손실 함수 정의"
      ],
      "metadata": {
        "id": "t-sFSp3wDZv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 정의\n",
        "class SCCELoss(Loss):\n",
        "    def __init__(self):\n",
        "        super(SCCELoss, self).__init__()\n",
        "        self.scce_loss = SparseCategoricalCrossentropy(\n",
        "            from_logits=False,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        '''\n",
        "        y_true: (batch_size, sequence_len)\n",
        "        y_pred: (batch_size, sequence_len, vocab_size)\n",
        "        '''\n",
        "        y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "        # 패딩 값은 loss 계산 과정에서 제외해야 함!!\n",
        "        mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)   # (batch_size, sequence_len)\n",
        "\n",
        "        loss = self.scce_loss(y_true, y_pred)  # (batch_size, sequence_len)\n",
        "        loss = tf.multiply(loss, mask)\n",
        "\n",
        "        return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "2Q2ZV86RAblj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = SCCELoss()"
      ],
      "metadata": {
        "id": "uoyhJvlVcK_y"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 최적화 정의"
      ],
      "metadata": {
        "id": "Ap_QbCfHDdY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케쥴러 정의\n",
        "# 해당 클래스를 상속받을 때는, 사전에 실수 형으로 cast해야 런타임 에러를 방지할 수 있음.\n",
        "class CustomSchedule(LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int = 512,\n",
        "        warmup_steps: int = 5000\n",
        "    ):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
        "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step_float = tf.cast(step, tf.float32)\n",
        "        arg1 = rsqrt(step_float)\n",
        "        arg2 = step_float * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return rsqrt(self.embed_dim) * minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "oDYC-O-nDfqW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = CustomSchedule(embed_dim=512)"
      ],
      "metadata": {
        "id": "QbZCO5j3e7A2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화 함수 정의\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=scheduler,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.98,\n",
        "    epsilon=1e-9\n",
        ")"
      ],
      "metadata": {
        "id": "B8U0i1wYCa2h"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 평가 지표 정의"
      ],
      "metadata": {
        "id": "XUHkSHP_jv53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = SparseCategoricalAccuracy()"
      ],
      "metadata": {
        "id": "kXgE7zWfl4Nn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 함수 정의"
      ],
      "metadata": {
        "id": "omaxM1o5MiJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    epochs,\n",
        "    model,\n",
        "    dataset,\n",
        "    loss,\n",
        "    optimizer,\n",
        "    metric\n",
        "):\n",
        "    global_step = 0\n",
        "    for epoch in range(epochs):\n",
        "        for step, data in enumerate(dataset):\n",
        "            enc_inputs = data['inputs']        # 인코더 입력 (batch_size, sequence_len)\n",
        "            dec_inputs = data['dec_inputs']    # 디코더 입력 (batch_size, sequence_len)\n",
        "            answers = data['outputs']          # 레이블 (batch_size, sequence_len)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # transformer 통과\n",
        "                probs = model(inputs=[enc_inputs, dec_inputs])  # (batch_size, sequence_len, vocab_size)\n",
        "\n",
        "                # loss 계산\n",
        "                losses = loss(answers, probs)\n",
        "\n",
        "            # 역전파 수행\n",
        "            trainable_vars = model.trainable_variables\n",
        "            gradients = tape.gradient(losses, trainable_vars)\n",
        "\n",
        "            # 파라미터 업데이트\n",
        "            optimizer.apply_gradients(\n",
        "                (grad, var)\n",
        "                for (grad, var) in zip(gradients, model.trainable_variables)\n",
        "                if grad is not None\n",
        "            )\n",
        "\n",
        "            if not step % 10:\n",
        "                # 지표 계산\n",
        "                answers = answers[:, :, tf.newaxis]  # (batch_size, sequence_len, 1)\n",
        "                metrics = metric(answers, probs).numpy()\n",
        "\n",
        "                # 동적 학습률 계산\n",
        "                lr = optimizer.learning_rate.numpy()\n",
        "\n",
        "                print(\n",
        "                    f\"[{epoch}/{epochs} epochs][{step}/{len(dataset)} steps]\\t\"\n",
        "                    f\"lr {lr:.4f}\\t\"\n",
        "                    f\"train loss {losses.numpy():.4f}\\t\"\n",
        "                    f\"train acc {metrics:.4f}\"\n",
        "                )\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        print(\"=\" * 100)\n",
        ""
      ],
      "metadata": {
        "id": "fnSnEXBK6Htq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 수행"
      ],
      "metadata": {
        "id": "tN2KV_ZOMj4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    epochs=30,\n",
        "    model=model,\n",
        "    dataset=dataset,\n",
        "    loss=loss,\n",
        "    optimizer=optimizer,\n",
        "    metric=metric\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnpHS6MDMl3k",
        "outputId": "12291db4-6cf9-4afd-be3f-b93ae3d06be9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7d7763eb7520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7d7763eb7520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/30 epochs][0/185 steps]\tlr 0.0000\ttrain loss 1.5289\ttrain acc 0.0000\n",
            "[0/30 epochs][10/185 steps]\tlr 0.0000\ttrain loss 1.5486\ttrain acc 0.0000\n",
            "[0/30 epochs][20/185 steps]\tlr 0.0000\ttrain loss 1.6095\ttrain acc 0.0000\n",
            "[0/30 epochs][30/185 steps]\tlr 0.0000\ttrain loss 1.3043\ttrain acc 0.0064\n",
            "[0/30 epochs][40/185 steps]\tlr 0.0000\ttrain loss 1.3980\ttrain acc 0.0103\n",
            "[0/30 epochs][50/185 steps]\tlr 0.0000\ttrain loss 1.3447\ttrain acc 0.0128\n",
            "[0/30 epochs][60/185 steps]\tlr 0.0000\ttrain loss 1.1421\ttrain acc 0.0147\n",
            "[0/30 epochs][70/185 steps]\tlr 0.0000\ttrain loss 1.2904\ttrain acc 0.0178\n",
            "[0/30 epochs][80/185 steps]\tlr 0.0000\ttrain loss 1.2641\ttrain acc 0.0187\n",
            "[0/30 epochs][90/185 steps]\tlr 0.0000\ttrain loss 1.3205\ttrain acc 0.0194\n",
            "[0/30 epochs][100/185 steps]\tlr 0.0000\ttrain loss 1.3184\ttrain acc 0.0201\n",
            "[0/30 epochs][110/185 steps]\tlr 0.0000\ttrain loss 1.2101\ttrain acc 0.0208\n",
            "[0/30 epochs][120/185 steps]\tlr 0.0000\ttrain loss 1.3468\ttrain acc 0.0218\n",
            "[0/30 epochs][130/185 steps]\tlr 0.0000\ttrain loss 1.2096\ttrain acc 0.0224\n",
            "[0/30 epochs][140/185 steps]\tlr 0.0000\ttrain loss 1.1730\ttrain acc 0.0230\n",
            "[0/30 epochs][150/185 steps]\tlr 0.0000\ttrain loss 1.0456\ttrain acc 0.0238\n",
            "[0/30 epochs][160/185 steps]\tlr 0.0000\ttrain loss 1.1680\ttrain acc 0.0250\n",
            "[0/30 epochs][170/185 steps]\tlr 0.0000\ttrain loss 1.0185\ttrain acc 0.0263\n",
            "[0/30 epochs][180/185 steps]\tlr 0.0000\ttrain loss 1.1669\ttrain acc 0.0274\n",
            "====================================================================================================\n",
            "[1/30 epochs][0/185 steps]\tlr 0.0000\ttrain loss 1.0303\ttrain acc 0.0284\n",
            "[1/30 epochs][10/185 steps]\tlr 0.0000\ttrain loss 1.0405\ttrain acc 0.0295\n",
            "[1/30 epochs][20/185 steps]\tlr 0.0000\ttrain loss 1.0541\ttrain acc 0.0305\n",
            "[1/30 epochs][30/185 steps]\tlr 0.0000\ttrain loss 0.9922\ttrain acc 0.0313\n",
            "[1/30 epochs][40/185 steps]\tlr 0.0000\ttrain loss 1.1392\ttrain acc 0.0321\n",
            "[1/30 epochs][50/185 steps]\tlr 0.0000\ttrain loss 1.0076\ttrain acc 0.0329\n",
            "[1/30 epochs][60/185 steps]\tlr 0.0000\ttrain loss 0.9496\ttrain acc 0.0336\n",
            "[1/30 epochs][70/185 steps]\tlr 0.0000\ttrain loss 1.1168\ttrain acc 0.0343\n",
            "[1/30 epochs][80/185 steps]\tlr 0.0000\ttrain loss 1.1038\ttrain acc 0.0348\n",
            "[1/30 epochs][90/185 steps]\tlr 0.0000\ttrain loss 0.9620\ttrain acc 0.0354\n",
            "[1/30 epochs][100/185 steps]\tlr 0.0000\ttrain loss 0.9810\ttrain acc 0.0360\n",
            "[1/30 epochs][110/185 steps]\tlr 0.0000\ttrain loss 0.9333\ttrain acc 0.0365\n",
            "[1/30 epochs][120/185 steps]\tlr 0.0000\ttrain loss 0.9844\ttrain acc 0.0371\n",
            "[1/30 epochs][130/185 steps]\tlr 0.0000\ttrain loss 1.0472\ttrain acc 0.0373\n",
            "[1/30 epochs][140/185 steps]\tlr 0.0000\ttrain loss 0.9894\ttrain acc 0.0376\n",
            "[1/30 epochs][150/185 steps]\tlr 0.0000\ttrain loss 1.0110\ttrain acc 0.0380\n",
            "[1/30 epochs][160/185 steps]\tlr 0.0000\ttrain loss 1.0416\ttrain acc 0.0384\n",
            "[1/30 epochs][170/185 steps]\tlr 0.0000\ttrain loss 1.0219\ttrain acc 0.0388\n",
            "[1/30 epochs][180/185 steps]\tlr 0.0000\ttrain loss 1.0545\ttrain acc 0.0392\n",
            "====================================================================================================\n",
            "[2/30 epochs][0/185 steps]\tlr 0.0000\ttrain loss 1.0507\ttrain acc 0.0396\n",
            "[2/30 epochs][10/185 steps]\tlr 0.0000\ttrain loss 1.0570\ttrain acc 0.0400\n",
            "[2/30 epochs][20/185 steps]\tlr 0.0000\ttrain loss 0.8707\ttrain acc 0.0403\n",
            "[2/30 epochs][30/185 steps]\tlr 0.0000\ttrain loss 1.0261\ttrain acc 0.0407\n",
            "[2/30 epochs][40/185 steps]\tlr 0.0001\ttrain loss 0.9130\ttrain acc 0.0410\n",
            "[2/30 epochs][50/185 steps]\tlr 0.0001\ttrain loss 0.9895\ttrain acc 0.0413\n",
            "[2/30 epochs][60/185 steps]\tlr 0.0001\ttrain loss 0.9569\ttrain acc 0.0416\n",
            "[2/30 epochs][70/185 steps]\tlr 0.0001\ttrain loss 1.0223\ttrain acc 0.0418\n",
            "[2/30 epochs][80/185 steps]\tlr 0.0001\ttrain loss 0.9702\ttrain acc 0.0421\n",
            "[2/30 epochs][90/185 steps]\tlr 0.0001\ttrain loss 1.0167\ttrain acc 0.0423\n",
            "[2/30 epochs][100/185 steps]\tlr 0.0001\ttrain loss 1.0117\ttrain acc 0.0426\n",
            "[2/30 epochs][110/185 steps]\tlr 0.0001\ttrain loss 0.8917\ttrain acc 0.0428\n",
            "[2/30 epochs][120/185 steps]\tlr 0.0001\ttrain loss 0.9863\ttrain acc 0.0430\n",
            "[2/30 epochs][130/185 steps]\tlr 0.0001\ttrain loss 0.8432\ttrain acc 0.0432\n",
            "[2/30 epochs][140/185 steps]\tlr 0.0001\ttrain loss 1.0075\ttrain acc 0.0434\n",
            "[2/30 epochs][150/185 steps]\tlr 0.0001\ttrain loss 1.0515\ttrain acc 0.0436\n",
            "[2/30 epochs][160/185 steps]\tlr 0.0001\ttrain loss 0.9732\ttrain acc 0.0438\n",
            "[2/30 epochs][170/185 steps]\tlr 0.0001\ttrain loss 0.9758\ttrain acc 0.0440\n",
            "[2/30 epochs][180/185 steps]\tlr 0.0001\ttrain loss 0.9769\ttrain acc 0.0442\n",
            "====================================================================================================\n",
            "[3/30 epochs][0/185 steps]\tlr 0.0001\ttrain loss 0.9969\ttrain acc 0.0444\n",
            "[3/30 epochs][10/185 steps]\tlr 0.0001\ttrain loss 0.9351\ttrain acc 0.0445\n",
            "[3/30 epochs][20/185 steps]\tlr 0.0001\ttrain loss 0.9726\ttrain acc 0.0446\n",
            "[3/30 epochs][30/185 steps]\tlr 0.0001\ttrain loss 1.0033\ttrain acc 0.0448\n",
            "[3/30 epochs][40/185 steps]\tlr 0.0001\ttrain loss 0.9467\ttrain acc 0.0449\n",
            "[3/30 epochs][50/185 steps]\tlr 0.0001\ttrain loss 0.8985\ttrain acc 0.0451\n",
            "[3/30 epochs][60/185 steps]\tlr 0.0001\ttrain loss 0.8593\ttrain acc 0.0453\n",
            "[3/30 epochs][70/185 steps]\tlr 0.0001\ttrain loss 0.9871\ttrain acc 0.0454\n",
            "[3/30 epochs][80/185 steps]\tlr 0.0001\ttrain loss 0.9428\ttrain acc 0.0456\n",
            "[3/30 epochs][90/185 steps]\tlr 0.0001\ttrain loss 0.9478\ttrain acc 0.0457\n",
            "[3/30 epochs][100/185 steps]\tlr 0.0001\ttrain loss 0.8582\ttrain acc 0.0459\n",
            "[3/30 epochs][110/185 steps]\tlr 0.0001\ttrain loss 0.9188\ttrain acc 0.0460\n",
            "[3/30 epochs][120/185 steps]\tlr 0.0001\ttrain loss 1.0455\ttrain acc 0.0461\n",
            "[3/30 epochs][130/185 steps]\tlr 0.0001\ttrain loss 1.0813\ttrain acc 0.0462\n",
            "[3/30 epochs][140/185 steps]\tlr 0.0001\ttrain loss 0.8450\ttrain acc 0.0464\n",
            "[3/30 epochs][150/185 steps]\tlr 0.0001\ttrain loss 0.9223\ttrain acc 0.0465\n",
            "[3/30 epochs][160/185 steps]\tlr 0.0001\ttrain loss 0.8746\ttrain acc 0.0466\n",
            "[3/30 epochs][170/185 steps]\tlr 0.0001\ttrain loss 0.9418\ttrain acc 0.0467\n",
            "[3/30 epochs][180/185 steps]\tlr 0.0001\ttrain loss 0.8568\ttrain acc 0.0468\n",
            "====================================================================================================\n",
            "[4/30 epochs][0/185 steps]\tlr 0.0001\ttrain loss 0.8500\ttrain acc 0.0469\n",
            "[4/30 epochs][10/185 steps]\tlr 0.0001\ttrain loss 0.8606\ttrain acc 0.0470\n",
            "[4/30 epochs][20/185 steps]\tlr 0.0001\ttrain loss 0.9413\ttrain acc 0.0471\n",
            "[4/30 epochs][30/185 steps]\tlr 0.0001\ttrain loss 0.7756\ttrain acc 0.0472\n",
            "[4/30 epochs][40/185 steps]\tlr 0.0001\ttrain loss 0.8461\ttrain acc 0.0473\n",
            "[4/30 epochs][50/185 steps]\tlr 0.0001\ttrain loss 0.9522\ttrain acc 0.0474\n",
            "[4/30 epochs][60/185 steps]\tlr 0.0001\ttrain loss 0.8796\ttrain acc 0.0475\n",
            "[4/30 epochs][70/185 steps]\tlr 0.0001\ttrain loss 0.8849\ttrain acc 0.0476\n",
            "[4/30 epochs][80/185 steps]\tlr 0.0001\ttrain loss 0.9725\ttrain acc 0.0477\n",
            "[4/30 epochs][90/185 steps]\tlr 0.0001\ttrain loss 0.7666\ttrain acc 0.0478\n",
            "[4/30 epochs][100/185 steps]\tlr 0.0001\ttrain loss 0.9037\ttrain acc 0.0479\n",
            "[4/30 epochs][110/185 steps]\tlr 0.0001\ttrain loss 0.8495\ttrain acc 0.0479\n",
            "[4/30 epochs][120/185 steps]\tlr 0.0001\ttrain loss 0.9798\ttrain acc 0.0480\n",
            "[4/30 epochs][130/185 steps]\tlr 0.0001\ttrain loss 0.8701\ttrain acc 0.0481\n",
            "[4/30 epochs][140/185 steps]\tlr 0.0001\ttrain loss 0.8686\ttrain acc 0.0481\n",
            "[4/30 epochs][150/185 steps]\tlr 0.0001\ttrain loss 1.0079\ttrain acc 0.0482\n",
            "[4/30 epochs][160/185 steps]\tlr 0.0001\ttrain loss 0.9081\ttrain acc 0.0483\n",
            "[4/30 epochs][170/185 steps]\tlr 0.0001\ttrain loss 0.7554\ttrain acc 0.0483\n",
            "[4/30 epochs][180/185 steps]\tlr 0.0001\ttrain loss 0.8661\ttrain acc 0.0484\n",
            "====================================================================================================\n",
            "[5/30 epochs][0/185 steps]\tlr 0.0001\ttrain loss 0.9731\ttrain acc 0.0485\n",
            "[5/30 epochs][10/185 steps]\tlr 0.0001\ttrain loss 0.8335\ttrain acc 0.0485\n",
            "[5/30 epochs][20/185 steps]\tlr 0.0001\ttrain loss 0.8851\ttrain acc 0.0486\n",
            "[5/30 epochs][30/185 steps]\tlr 0.0001\ttrain loss 0.8707\ttrain acc 0.0487\n",
            "[5/30 epochs][40/185 steps]\tlr 0.0001\ttrain loss 0.7476\ttrain acc 0.0487\n",
            "[5/30 epochs][50/185 steps]\tlr 0.0001\ttrain loss 0.8594\ttrain acc 0.0488\n",
            "[5/30 epochs][60/185 steps]\tlr 0.0001\ttrain loss 0.8181\ttrain acc 0.0488\n",
            "[5/30 epochs][70/185 steps]\tlr 0.0001\ttrain loss 0.8813\ttrain acc 0.0489\n",
            "[5/30 epochs][80/185 steps]\tlr 0.0001\ttrain loss 0.8129\ttrain acc 0.0489\n",
            "[5/30 epochs][90/185 steps]\tlr 0.0001\ttrain loss 0.8881\ttrain acc 0.0490\n",
            "[5/30 epochs][100/185 steps]\tlr 0.0001\ttrain loss 0.9162\ttrain acc 0.0490\n",
            "[5/30 epochs][110/185 steps]\tlr 0.0001\ttrain loss 0.9168\ttrain acc 0.0491\n",
            "[5/30 epochs][120/185 steps]\tlr 0.0001\ttrain loss 0.7701\ttrain acc 0.0491\n",
            "[5/30 epochs][130/185 steps]\tlr 0.0001\ttrain loss 0.8512\ttrain acc 0.0492\n",
            "[5/30 epochs][140/185 steps]\tlr 0.0001\ttrain loss 0.9584\ttrain acc 0.0492\n",
            "[5/30 epochs][150/185 steps]\tlr 0.0001\ttrain loss 0.9317\ttrain acc 0.0493\n",
            "[5/30 epochs][160/185 steps]\tlr 0.0001\ttrain loss 0.9940\ttrain acc 0.0493\n",
            "[5/30 epochs][170/185 steps]\tlr 0.0001\ttrain loss 0.8560\ttrain acc 0.0493\n",
            "[5/30 epochs][180/185 steps]\tlr 0.0001\ttrain loss 0.9540\ttrain acc 0.0493\n",
            "====================================================================================================\n",
            "[6/30 epochs][0/185 steps]\tlr 0.0001\ttrain loss 0.7496\ttrain acc 0.0493\n",
            "[6/30 epochs][10/185 steps]\tlr 0.0001\ttrain loss 0.9223\ttrain acc 0.0494\n",
            "[6/30 epochs][20/185 steps]\tlr 0.0001\ttrain loss 0.8627\ttrain acc 0.0494\n",
            "[6/30 epochs][30/185 steps]\tlr 0.0001\ttrain loss 0.8050\ttrain acc 0.0495\n",
            "[6/30 epochs][40/185 steps]\tlr 0.0001\ttrain loss 0.9224\ttrain acc 0.0495\n",
            "[6/30 epochs][50/185 steps]\tlr 0.0001\ttrain loss 0.8482\ttrain acc 0.0496\n",
            "[6/30 epochs][60/185 steps]\tlr 0.0001\ttrain loss 0.9293\ttrain acc 0.0496\n",
            "[6/30 epochs][70/185 steps]\tlr 0.0001\ttrain loss 0.8912\ttrain acc 0.0496\n",
            "[6/30 epochs][80/185 steps]\tlr 0.0001\ttrain loss 0.8889\ttrain acc 0.0497\n",
            "[6/30 epochs][90/185 steps]\tlr 0.0001\ttrain loss 0.8569\ttrain acc 0.0497\n",
            "[6/30 epochs][100/185 steps]\tlr 0.0002\ttrain loss 0.8891\ttrain acc 0.0498\n",
            "[6/30 epochs][110/185 steps]\tlr 0.0002\ttrain loss 0.8208\ttrain acc 0.0499\n",
            "[6/30 epochs][120/185 steps]\tlr 0.0002\ttrain loss 1.0537\ttrain acc 0.0499\n",
            "[6/30 epochs][130/185 steps]\tlr 0.0002\ttrain loss 0.8390\ttrain acc 0.0499\n",
            "[6/30 epochs][140/185 steps]\tlr 0.0002\ttrain loss 0.8892\ttrain acc 0.0499\n",
            "[6/30 epochs][150/185 steps]\tlr 0.0002\ttrain loss 0.8923\ttrain acc 0.0500\n",
            "[6/30 epochs][160/185 steps]\tlr 0.0002\ttrain loss 0.9444\ttrain acc 0.0500\n",
            "[6/30 epochs][170/185 steps]\tlr 0.0002\ttrain loss 0.9543\ttrain acc 0.0500\n",
            "[6/30 epochs][180/185 steps]\tlr 0.0002\ttrain loss 0.9016\ttrain acc 0.0501\n",
            "====================================================================================================\n",
            "[7/30 epochs][0/185 steps]\tlr 0.0002\ttrain loss 0.8890\ttrain acc 0.0501\n",
            "[7/30 epochs][10/185 steps]\tlr 0.0002\ttrain loss 0.8097\ttrain acc 0.0501\n",
            "[7/30 epochs][20/185 steps]\tlr 0.0002\ttrain loss 0.8366\ttrain acc 0.0502\n",
            "[7/30 epochs][30/185 steps]\tlr 0.0002\ttrain loss 0.8818\ttrain acc 0.0502\n",
            "[7/30 epochs][40/185 steps]\tlr 0.0002\ttrain loss 0.7614\ttrain acc 0.0502\n",
            "[7/30 epochs][50/185 steps]\tlr 0.0002\ttrain loss 0.8168\ttrain acc 0.0503\n",
            "[7/30 epochs][60/185 steps]\tlr 0.0002\ttrain loss 0.9023\ttrain acc 0.0503\n",
            "[7/30 epochs][70/185 steps]\tlr 0.0002\ttrain loss 0.8310\ttrain acc 0.0503\n",
            "[7/30 epochs][80/185 steps]\tlr 0.0002\ttrain loss 0.8908\ttrain acc 0.0503\n",
            "[7/30 epochs][90/185 steps]\tlr 0.0002\ttrain loss 0.8929\ttrain acc 0.0502\n",
            "[7/30 epochs][100/185 steps]\tlr 0.0002\ttrain loss 0.9110\ttrain acc 0.0503\n",
            "[7/30 epochs][110/185 steps]\tlr 0.0002\ttrain loss 0.9194\ttrain acc 0.0503\n",
            "[7/30 epochs][120/185 steps]\tlr 0.0002\ttrain loss 0.8991\ttrain acc 0.0503\n",
            "[7/30 epochs][130/185 steps]\tlr 0.0002\ttrain loss 0.9550\ttrain acc 0.0504\n",
            "[7/30 epochs][140/185 steps]\tlr 0.0002\ttrain loss 0.8960\ttrain acc 0.0504\n",
            "[7/30 epochs][150/185 steps]\tlr 0.0002\ttrain loss 0.8269\ttrain acc 0.0504\n",
            "[7/30 epochs][160/185 steps]\tlr 0.0002\ttrain loss 0.9222\ttrain acc 0.0504\n",
            "[7/30 epochs][170/185 steps]\tlr 0.0002\ttrain loss 0.7389\ttrain acc 0.0505\n",
            "[7/30 epochs][180/185 steps]\tlr 0.0002\ttrain loss 0.8962\ttrain acc 0.0505\n",
            "====================================================================================================\n",
            "[8/30 epochs][0/185 steps]\tlr 0.0002\ttrain loss 0.8510\ttrain acc 0.0505\n",
            "[8/30 epochs][10/185 steps]\tlr 0.0002\ttrain loss 0.9966\ttrain acc 0.0506\n",
            "[8/30 epochs][20/185 steps]\tlr 0.0002\ttrain loss 0.9529\ttrain acc 0.0506\n",
            "[8/30 epochs][30/185 steps]\tlr 0.0002\ttrain loss 0.7871\ttrain acc 0.0506\n",
            "[8/30 epochs][40/185 steps]\tlr 0.0002\ttrain loss 0.8242\ttrain acc 0.0506\n",
            "[8/30 epochs][50/185 steps]\tlr 0.0002\ttrain loss 0.8798\ttrain acc 0.0507\n",
            "[8/30 epochs][60/185 steps]\tlr 0.0002\ttrain loss 0.8870\ttrain acc 0.0507\n",
            "[8/30 epochs][70/185 steps]\tlr 0.0002\ttrain loss 0.9706\ttrain acc 0.0507\n",
            "[8/30 epochs][80/185 steps]\tlr 0.0002\ttrain loss 0.8665\ttrain acc 0.0507\n",
            "[8/30 epochs][90/185 steps]\tlr 0.0002\ttrain loss 0.9926\ttrain acc 0.0508\n",
            "[8/30 epochs][100/185 steps]\tlr 0.0002\ttrain loss 0.8206\ttrain acc 0.0508\n",
            "[8/30 epochs][110/185 steps]\tlr 0.0002\ttrain loss 0.8740\ttrain acc 0.0508\n",
            "[8/30 epochs][120/185 steps]\tlr 0.0002\ttrain loss 0.8520\ttrain acc 0.0509\n",
            "[8/30 epochs][130/185 steps]\tlr 0.0002\ttrain loss 0.9287\ttrain acc 0.0509\n",
            "[8/30 epochs][140/185 steps]\tlr 0.0002\ttrain loss 1.0389\ttrain acc 0.0508\n",
            "[8/30 epochs][150/185 steps]\tlr 0.0002\ttrain loss 0.7952\ttrain acc 0.0508\n",
            "[8/30 epochs][160/185 steps]\tlr 0.0002\ttrain loss 0.8943\ttrain acc 0.0508\n",
            "[8/30 epochs][170/185 steps]\tlr 0.0002\ttrain loss 0.8979\ttrain acc 0.0508\n",
            "[8/30 epochs][180/185 steps]\tlr 0.0002\ttrain loss 0.9245\ttrain acc 0.0509\n",
            "====================================================================================================\n",
            "[9/30 epochs][0/185 steps]\tlr 0.0002\ttrain loss 0.8028\ttrain acc 0.0509\n",
            "[9/30 epochs][10/185 steps]\tlr 0.0002\ttrain loss 0.9456\ttrain acc 0.0509\n",
            "[9/30 epochs][20/185 steps]\tlr 0.0002\ttrain loss 0.8339\ttrain acc 0.0509\n",
            "[9/30 epochs][30/185 steps]\tlr 0.0002\ttrain loss 0.8938\ttrain acc 0.0509\n",
            "[9/30 epochs][40/185 steps]\tlr 0.0002\ttrain loss 0.8367\ttrain acc 0.0509\n",
            "[9/30 epochs][50/185 steps]\tlr 0.0002\ttrain loss 0.7791\ttrain acc 0.0509\n",
            "[9/30 epochs][60/185 steps]\tlr 0.0002\ttrain loss 0.8221\ttrain acc 0.0510\n",
            "[9/30 epochs][70/185 steps]\tlr 0.0002\ttrain loss 1.0115\ttrain acc 0.0510\n",
            "[9/30 epochs][80/185 steps]\tlr 0.0002\ttrain loss 0.8731\ttrain acc 0.0510\n",
            "[9/30 epochs][90/185 steps]\tlr 0.0002\ttrain loss 0.9029\ttrain acc 0.0510\n",
            "[9/30 epochs][100/185 steps]\tlr 0.0002\ttrain loss 0.7921\ttrain acc 0.0510\n",
            "[9/30 epochs][110/185 steps]\tlr 0.0002\ttrain loss 0.8687\ttrain acc 0.0511\n",
            "[9/30 epochs][120/185 steps]\tlr 0.0002\ttrain loss 0.8324\ttrain acc 0.0511\n",
            "[9/30 epochs][130/185 steps]\tlr 0.0002\ttrain loss 0.9586\ttrain acc 0.0511\n",
            "[9/30 epochs][140/185 steps]\tlr 0.0002\ttrain loss 0.8236\ttrain acc 0.0511\n",
            "[9/30 epochs][150/185 steps]\tlr 0.0002\ttrain loss 0.7798\ttrain acc 0.0511\n",
            "[9/30 epochs][160/185 steps]\tlr 0.0002\ttrain loss 0.8618\ttrain acc 0.0512\n",
            "[9/30 epochs][170/185 steps]\tlr 0.0002\ttrain loss 0.7589\ttrain acc 0.0512\n",
            "[9/30 epochs][180/185 steps]\tlr 0.0002\ttrain loss 0.8068\ttrain acc 0.0512\n",
            "====================================================================================================\n",
            "[10/30 epochs][0/185 steps]\tlr 0.0002\ttrain loss 0.7836\ttrain acc 0.0512\n",
            "[10/30 epochs][10/185 steps]\tlr 0.0002\ttrain loss 0.7101\ttrain acc 0.0512\n",
            "[10/30 epochs][20/185 steps]\tlr 0.0002\ttrain loss 0.9050\ttrain acc 0.0513\n",
            "[10/30 epochs][30/185 steps]\tlr 0.0002\ttrain loss 0.8562\ttrain acc 0.0513\n",
            "[10/30 epochs][40/185 steps]\tlr 0.0002\ttrain loss 0.7851\ttrain acc 0.0512\n",
            "[10/30 epochs][50/185 steps]\tlr 0.0002\ttrain loss 0.7850\ttrain acc 0.0513\n",
            "[10/30 epochs][60/185 steps]\tlr 0.0002\ttrain loss 0.8245\ttrain acc 0.0513\n",
            "[10/30 epochs][70/185 steps]\tlr 0.0002\ttrain loss 0.8493\ttrain acc 0.0513\n",
            "[10/30 epochs][80/185 steps]\tlr 0.0002\ttrain loss 0.7973\ttrain acc 0.0513\n",
            "[10/30 epochs][90/185 steps]\tlr 0.0002\ttrain loss 0.8092\ttrain acc 0.0513\n",
            "[10/30 epochs][100/185 steps]\tlr 0.0002\ttrain loss 0.8579\ttrain acc 0.0513\n",
            "[10/30 epochs][110/185 steps]\tlr 0.0002\ttrain loss 0.8237\ttrain acc 0.0514\n",
            "[10/30 epochs][120/185 steps]\tlr 0.0002\ttrain loss 0.7331\ttrain acc 0.0514\n",
            "[10/30 epochs][130/185 steps]\tlr 0.0002\ttrain loss 1.0131\ttrain acc 0.0513\n",
            "[10/30 epochs][140/185 steps]\tlr 0.0002\ttrain loss 0.9872\ttrain acc 0.0513\n",
            "[10/30 epochs][150/185 steps]\tlr 0.0002\ttrain loss 0.8038\ttrain acc 0.0513\n",
            "[10/30 epochs][160/185 steps]\tlr 0.0003\ttrain loss 0.7575\ttrain acc 0.0513\n",
            "[10/30 epochs][170/185 steps]\tlr 0.0003\ttrain loss 0.7386\ttrain acc 0.0514\n",
            "[10/30 epochs][180/185 steps]\tlr 0.0003\ttrain loss 0.8052\ttrain acc 0.0514\n",
            "====================================================================================================\n",
            "[11/30 epochs][0/185 steps]\tlr 0.0003\ttrain loss 0.7744\ttrain acc 0.0514\n",
            "[11/30 epochs][10/185 steps]\tlr 0.0003\ttrain loss 0.7882\ttrain acc 0.0514\n",
            "[11/30 epochs][20/185 steps]\tlr 0.0003\ttrain loss 0.7971\ttrain acc 0.0514\n",
            "[11/30 epochs][30/185 steps]\tlr 0.0003\ttrain loss 0.7729\ttrain acc 0.0514\n",
            "[11/30 epochs][40/185 steps]\tlr 0.0003\ttrain loss 0.8316\ttrain acc 0.0515\n",
            "[11/30 epochs][50/185 steps]\tlr 0.0003\ttrain loss 0.8448\ttrain acc 0.0515\n",
            "[11/30 epochs][60/185 steps]\tlr 0.0003\ttrain loss 0.8265\ttrain acc 0.0515\n",
            "[11/30 epochs][70/185 steps]\tlr 0.0003\ttrain loss 0.7468\ttrain acc 0.0515\n",
            "[11/30 epochs][80/185 steps]\tlr 0.0003\ttrain loss 0.8318\ttrain acc 0.0515\n",
            "[11/30 epochs][90/185 steps]\tlr 0.0003\ttrain loss 0.8758\ttrain acc 0.0515\n",
            "[11/30 epochs][100/185 steps]\tlr 0.0003\ttrain loss 0.9322\ttrain acc 0.0515\n",
            "[11/30 epochs][110/185 steps]\tlr 0.0003\ttrain loss 0.8684\ttrain acc 0.0515\n",
            "[11/30 epochs][120/185 steps]\tlr 0.0003\ttrain loss 0.8288\ttrain acc 0.0515\n",
            "[11/30 epochs][130/185 steps]\tlr 0.0003\ttrain loss 0.9276\ttrain acc 0.0515\n",
            "[11/30 epochs][140/185 steps]\tlr 0.0003\ttrain loss 0.9456\ttrain acc 0.0516\n",
            "[11/30 epochs][150/185 steps]\tlr 0.0003\ttrain loss 0.8918\ttrain acc 0.0516\n",
            "[11/30 epochs][160/185 steps]\tlr 0.0003\ttrain loss 0.7250\ttrain acc 0.0516\n",
            "[11/30 epochs][170/185 steps]\tlr 0.0003\ttrain loss 0.7892\ttrain acc 0.0516\n",
            "[11/30 epochs][180/185 steps]\tlr 0.0003\ttrain loss 0.9395\ttrain acc 0.0516\n",
            "====================================================================================================\n",
            "[12/30 epochs][0/185 steps]\tlr 0.0003\ttrain loss 0.8504\ttrain acc 0.0516\n",
            "[12/30 epochs][10/185 steps]\tlr 0.0003\ttrain loss 0.7687\ttrain acc 0.0516\n",
            "[12/30 epochs][20/185 steps]\tlr 0.0003\ttrain loss 0.7753\ttrain acc 0.0516\n",
            "[12/30 epochs][30/185 steps]\tlr 0.0003\ttrain loss 0.7951\ttrain acc 0.0516\n",
            "[12/30 epochs][40/185 steps]\tlr 0.0003\ttrain loss 0.8381\ttrain acc 0.0516\n",
            "[12/30 epochs][50/185 steps]\tlr 0.0003\ttrain loss 0.8490\ttrain acc 0.0516\n",
            "[12/30 epochs][60/185 steps]\tlr 0.0003\ttrain loss 0.8906\ttrain acc 0.0516\n",
            "[12/30 epochs][70/185 steps]\tlr 0.0003\ttrain loss 0.7844\ttrain acc 0.0517\n",
            "[12/30 epochs][80/185 steps]\tlr 0.0003\ttrain loss 0.8446\ttrain acc 0.0517\n",
            "[12/30 epochs][90/185 steps]\tlr 0.0003\ttrain loss 0.8495\ttrain acc 0.0517\n",
            "[12/30 epochs][100/185 steps]\tlr 0.0003\ttrain loss 0.7449\ttrain acc 0.0517\n",
            "[12/30 epochs][110/185 steps]\tlr 0.0003\ttrain loss 0.7643\ttrain acc 0.0517\n",
            "[12/30 epochs][120/185 steps]\tlr 0.0003\ttrain loss 0.8100\ttrain acc 0.0517\n",
            "[12/30 epochs][130/185 steps]\tlr 0.0003\ttrain loss 0.8029\ttrain acc 0.0517\n",
            "[12/30 epochs][140/185 steps]\tlr 0.0003\ttrain loss 0.8527\ttrain acc 0.0518\n",
            "[12/30 epochs][150/185 steps]\tlr 0.0003\ttrain loss 0.9669\ttrain acc 0.0518\n",
            "[12/30 epochs][160/185 steps]\tlr 0.0003\ttrain loss 0.8340\ttrain acc 0.0518\n",
            "[12/30 epochs][170/185 steps]\tlr 0.0003\ttrain loss 0.8218\ttrain acc 0.0518\n",
            "[12/30 epochs][180/185 steps]\tlr 0.0003\ttrain loss 0.8101\ttrain acc 0.0518\n",
            "====================================================================================================\n",
            "[13/30 epochs][0/185 steps]\tlr 0.0003\ttrain loss 1.1092\ttrain acc 0.0517\n",
            "[13/30 epochs][10/185 steps]\tlr 0.0003\ttrain loss 0.8460\ttrain acc 0.0518\n",
            "[13/30 epochs][20/185 steps]\tlr 0.0003\ttrain loss 0.7104\ttrain acc 0.0518\n",
            "[13/30 epochs][30/185 steps]\tlr 0.0003\ttrain loss 0.8045\ttrain acc 0.0518\n",
            "[13/30 epochs][40/185 steps]\tlr 0.0003\ttrain loss 0.8467\ttrain acc 0.0518\n",
            "[13/30 epochs][50/185 steps]\tlr 0.0003\ttrain loss 0.7750\ttrain acc 0.0518\n",
            "[13/30 epochs][60/185 steps]\tlr 0.0003\ttrain loss 0.8304\ttrain acc 0.0518\n",
            "[13/30 epochs][70/185 steps]\tlr 0.0003\ttrain loss 0.8097\ttrain acc 0.0518\n",
            "[13/30 epochs][80/185 steps]\tlr 0.0003\ttrain loss 0.8121\ttrain acc 0.0518\n",
            "[13/30 epochs][90/185 steps]\tlr 0.0003\ttrain loss 0.8439\ttrain acc 0.0518\n",
            "[13/30 epochs][100/185 steps]\tlr 0.0003\ttrain loss 0.8106\ttrain acc 0.0518\n",
            "[13/30 epochs][110/185 steps]\tlr 0.0003\ttrain loss 0.7901\ttrain acc 0.0519\n",
            "[13/30 epochs][120/185 steps]\tlr 0.0003\ttrain loss 0.8395\ttrain acc 0.0519\n",
            "[13/30 epochs][130/185 steps]\tlr 0.0003\ttrain loss 0.9318\ttrain acc 0.0519\n",
            "[13/30 epochs][140/185 steps]\tlr 0.0003\ttrain loss 0.8613\ttrain acc 0.0519\n",
            "[13/30 epochs][150/185 steps]\tlr 0.0003\ttrain loss 0.8568\ttrain acc 0.0519\n",
            "[13/30 epochs][160/185 steps]\tlr 0.0003\ttrain loss 0.8873\ttrain acc 0.0519\n",
            "[13/30 epochs][170/185 steps]\tlr 0.0003\ttrain loss 0.8110\ttrain acc 0.0519\n",
            "[13/30 epochs][180/185 steps]\tlr 0.0003\ttrain loss 0.8481\ttrain acc 0.0519\n",
            "====================================================================================================\n",
            "[14/30 epochs][0/185 steps]\tlr 0.0003\ttrain loss 0.7816\ttrain acc 0.0519\n",
            "[14/30 epochs][10/185 steps]\tlr 0.0003\ttrain loss 0.8424\ttrain acc 0.0519\n",
            "[14/30 epochs][20/185 steps]\tlr 0.0003\ttrain loss 0.8735\ttrain acc 0.0519\n",
            "[14/30 epochs][30/185 steps]\tlr 0.0003\ttrain loss 0.8624\ttrain acc 0.0519\n",
            "[14/30 epochs][40/185 steps]\tlr 0.0003\ttrain loss 0.8478\ttrain acc 0.0519\n",
            "[14/30 epochs][50/185 steps]\tlr 0.0003\ttrain loss 0.7606\ttrain acc 0.0519\n",
            "[14/30 epochs][60/185 steps]\tlr 0.0003\ttrain loss 0.8016\ttrain acc 0.0519\n",
            "[14/30 epochs][70/185 steps]\tlr 0.0003\ttrain loss 0.7790\ttrain acc 0.0520\n",
            "[14/30 epochs][80/185 steps]\tlr 0.0003\ttrain loss 0.7551\ttrain acc 0.0520\n",
            "[14/30 epochs][90/185 steps]\tlr 0.0003\ttrain loss 0.8516\ttrain acc 0.0520\n",
            "[14/30 epochs][100/185 steps]\tlr 0.0003\ttrain loss 0.7696\ttrain acc 0.0520\n",
            "[14/30 epochs][110/185 steps]\tlr 0.0003\ttrain loss 0.8884\ttrain acc 0.0520\n",
            "[14/30 epochs][120/185 steps]\tlr 0.0003\ttrain loss 0.9821\ttrain acc 0.0520\n",
            "[14/30 epochs][130/185 steps]\tlr 0.0003\ttrain loss 0.9516\ttrain acc 0.0520\n",
            "[14/30 epochs][140/185 steps]\tlr 0.0003\ttrain loss 0.7572\ttrain acc 0.0520\n",
            "[14/30 epochs][150/185 steps]\tlr 0.0003\ttrain loss 0.7428\ttrain acc 0.0520\n",
            "[14/30 epochs][160/185 steps]\tlr 0.0003\ttrain loss 0.8024\ttrain acc 0.0521\n",
            "[14/30 epochs][170/185 steps]\tlr 0.0003\ttrain loss 0.8223\ttrain acc 0.0521\n",
            "[14/30 epochs][180/185 steps]\tlr 0.0003\ttrain loss 0.8732\ttrain acc 0.0521\n",
            "====================================================================================================\n",
            "[15/30 epochs][0/185 steps]\tlr 0.0003\ttrain loss 0.7999\ttrain acc 0.0521\n",
            "[15/30 epochs][10/185 steps]\tlr 0.0003\ttrain loss 0.8638\ttrain acc 0.0521\n",
            "[15/30 epochs][20/185 steps]\tlr 0.0003\ttrain loss 0.6744\ttrain acc 0.0521\n",
            "[15/30 epochs][30/185 steps]\tlr 0.0004\ttrain loss 0.8376\ttrain acc 0.0521\n",
            "[15/30 epochs][40/185 steps]\tlr 0.0004\ttrain loss 0.7817\ttrain acc 0.0521\n",
            "[15/30 epochs][50/185 steps]\tlr 0.0004\ttrain loss 0.7909\ttrain acc 0.0521\n",
            "[15/30 epochs][60/185 steps]\tlr 0.0004\ttrain loss 0.8485\ttrain acc 0.0521\n",
            "[15/30 epochs][70/185 steps]\tlr 0.0004\ttrain loss 0.8676\ttrain acc 0.0521\n",
            "[15/30 epochs][80/185 steps]\tlr 0.0004\ttrain loss 0.8253\ttrain acc 0.0521\n",
            "[15/30 epochs][90/185 steps]\tlr 0.0004\ttrain loss 0.7721\ttrain acc 0.0521\n",
            "[15/30 epochs][100/185 steps]\tlr 0.0004\ttrain loss 0.7286\ttrain acc 0.0521\n",
            "[15/30 epochs][110/185 steps]\tlr 0.0004\ttrain loss 0.8140\ttrain acc 0.0521\n",
            "[15/30 epochs][120/185 steps]\tlr 0.0004\ttrain loss 0.8226\ttrain acc 0.0521\n",
            "[15/30 epochs][130/185 steps]\tlr 0.0004\ttrain loss 0.8660\ttrain acc 0.0521\n",
            "[15/30 epochs][140/185 steps]\tlr 0.0004\ttrain loss 0.7898\ttrain acc 0.0521\n",
            "[15/30 epochs][150/185 steps]\tlr 0.0004\ttrain loss 0.8379\ttrain acc 0.0522\n",
            "[15/30 epochs][160/185 steps]\tlr 0.0004\ttrain loss 0.7650\ttrain acc 0.0522\n",
            "[15/30 epochs][170/185 steps]\tlr 0.0004\ttrain loss 0.7894\ttrain acc 0.0522\n",
            "[15/30 epochs][180/185 steps]\tlr 0.0004\ttrain loss 0.8920\ttrain acc 0.0522\n",
            "====================================================================================================\n",
            "[16/30 epochs][0/185 steps]\tlr 0.0004\ttrain loss 0.8603\ttrain acc 0.0522\n",
            "[16/30 epochs][10/185 steps]\tlr 0.0004\ttrain loss 0.8217\ttrain acc 0.0522\n",
            "[16/30 epochs][20/185 steps]\tlr 0.0004\ttrain loss 0.7534\ttrain acc 0.0522\n",
            "[16/30 epochs][30/185 steps]\tlr 0.0004\ttrain loss 0.7572\ttrain acc 0.0522\n",
            "[16/30 epochs][40/185 steps]\tlr 0.0004\ttrain loss 0.9056\ttrain acc 0.0522\n",
            "[16/30 epochs][50/185 steps]\tlr 0.0004\ttrain loss 0.7017\ttrain acc 0.0522\n",
            "[16/30 epochs][60/185 steps]\tlr 0.0004\ttrain loss 0.8353\ttrain acc 0.0522\n",
            "[16/30 epochs][70/185 steps]\tlr 0.0004\ttrain loss 0.9050\ttrain acc 0.0522\n",
            "[16/30 epochs][80/185 steps]\tlr 0.0004\ttrain loss 0.8542\ttrain acc 0.0522\n",
            "[16/30 epochs][90/185 steps]\tlr 0.0004\ttrain loss 0.9470\ttrain acc 0.0522\n",
            "[16/30 epochs][100/185 steps]\tlr 0.0004\ttrain loss 0.8244\ttrain acc 0.0522\n",
            "[16/30 epochs][110/185 steps]\tlr 0.0004\ttrain loss 0.8839\ttrain acc 0.0522\n",
            "[16/30 epochs][120/185 steps]\tlr 0.0004\ttrain loss 0.7607\ttrain acc 0.0522\n",
            "[16/30 epochs][130/185 steps]\tlr 0.0004\ttrain loss 0.8118\ttrain acc 0.0522\n",
            "[16/30 epochs][140/185 steps]\tlr 0.0004\ttrain loss 0.8446\ttrain acc 0.0522\n",
            "[16/30 epochs][150/185 steps]\tlr 0.0004\ttrain loss 0.8223\ttrain acc 0.0523\n",
            "[16/30 epochs][160/185 steps]\tlr 0.0004\ttrain loss 0.8101\ttrain acc 0.0523\n",
            "[16/30 epochs][170/185 steps]\tlr 0.0004\ttrain loss 0.8153\ttrain acc 0.0523\n",
            "[16/30 epochs][180/185 steps]\tlr 0.0004\ttrain loss 0.8059\ttrain acc 0.0523\n",
            "====================================================================================================\n",
            "[17/30 epochs][0/185 steps]\tlr 0.0004\ttrain loss 0.8091\ttrain acc 0.0523\n",
            "[17/30 epochs][10/185 steps]\tlr 0.0004\ttrain loss 0.7850\ttrain acc 0.0523\n",
            "[17/30 epochs][20/185 steps]\tlr 0.0004\ttrain loss 0.8634\ttrain acc 0.0523\n",
            "[17/30 epochs][30/185 steps]\tlr 0.0004\ttrain loss 0.8509\ttrain acc 0.0523\n",
            "[17/30 epochs][40/185 steps]\tlr 0.0004\ttrain loss 0.8203\ttrain acc 0.0523\n",
            "[17/30 epochs][50/185 steps]\tlr 0.0004\ttrain loss 0.7772\ttrain acc 0.0523\n",
            "[17/30 epochs][60/185 steps]\tlr 0.0004\ttrain loss 0.7381\ttrain acc 0.0523\n",
            "[17/30 epochs][70/185 steps]\tlr 0.0004\ttrain loss 0.8623\ttrain acc 0.0523\n",
            "[17/30 epochs][80/185 steps]\tlr 0.0004\ttrain loss 0.8613\ttrain acc 0.0523\n",
            "[17/30 epochs][90/185 steps]\tlr 0.0004\ttrain loss 0.8606\ttrain acc 0.0523\n",
            "[17/30 epochs][100/185 steps]\tlr 0.0004\ttrain loss 0.8217\ttrain acc 0.0523\n",
            "[17/30 epochs][110/185 steps]\tlr 0.0004\ttrain loss 0.8636\ttrain acc 0.0523\n",
            "[17/30 epochs][120/185 steps]\tlr 0.0004\ttrain loss 0.7789\ttrain acc 0.0523\n",
            "[17/30 epochs][130/185 steps]\tlr 0.0004\ttrain loss 0.8213\ttrain acc 0.0523\n",
            "[17/30 epochs][140/185 steps]\tlr 0.0004\ttrain loss 0.8343\ttrain acc 0.0524\n",
            "[17/30 epochs][150/185 steps]\tlr 0.0004\ttrain loss 0.7569\ttrain acc 0.0524\n",
            "[17/30 epochs][160/185 steps]\tlr 0.0004\ttrain loss 0.7870\ttrain acc 0.0524\n",
            "[17/30 epochs][170/185 steps]\tlr 0.0004\ttrain loss 0.8417\ttrain acc 0.0524\n",
            "[17/30 epochs][180/185 steps]\tlr 0.0004\ttrain loss 0.8974\ttrain acc 0.0524\n",
            "====================================================================================================\n",
            "[18/30 epochs][0/185 steps]\tlr 0.0004\ttrain loss 0.7522\ttrain acc 0.0524\n",
            "[18/30 epochs][10/185 steps]\tlr 0.0004\ttrain loss 0.8104\ttrain acc 0.0524\n",
            "[18/30 epochs][20/185 steps]\tlr 0.0004\ttrain loss 0.8540\ttrain acc 0.0524\n",
            "[18/30 epochs][30/185 steps]\tlr 0.0004\ttrain loss 0.8366\ttrain acc 0.0524\n",
            "[18/30 epochs][40/185 steps]\tlr 0.0004\ttrain loss 0.8332\ttrain acc 0.0524\n",
            "[18/30 epochs][50/185 steps]\tlr 0.0004\ttrain loss 1.0122\ttrain acc 0.0524\n",
            "[18/30 epochs][60/185 steps]\tlr 0.0004\ttrain loss 0.8305\ttrain acc 0.0524\n",
            "[18/30 epochs][70/185 steps]\tlr 0.0004\ttrain loss 0.7327\ttrain acc 0.0524\n",
            "[18/30 epochs][80/185 steps]\tlr 0.0004\ttrain loss 0.7780\ttrain acc 0.0524\n",
            "[18/30 epochs][90/185 steps]\tlr 0.0004\ttrain loss 0.7233\ttrain acc 0.0524\n",
            "[18/30 epochs][100/185 steps]\tlr 0.0004\ttrain loss 0.8399\ttrain acc 0.0524\n",
            "[18/30 epochs][110/185 steps]\tlr 0.0004\ttrain loss 0.7859\ttrain acc 0.0524\n",
            "[18/30 epochs][120/185 steps]\tlr 0.0004\ttrain loss 0.7707\ttrain acc 0.0524\n",
            "[18/30 epochs][130/185 steps]\tlr 0.0004\ttrain loss 0.9094\ttrain acc 0.0524\n",
            "[18/30 epochs][140/185 steps]\tlr 0.0004\ttrain loss 0.8845\ttrain acc 0.0524\n",
            "[18/30 epochs][150/185 steps]\tlr 0.0004\ttrain loss 0.9969\ttrain acc 0.0524\n",
            "[18/30 epochs][160/185 steps]\tlr 0.0004\ttrain loss 0.9005\ttrain acc 0.0524\n",
            "[18/30 epochs][170/185 steps]\tlr 0.0004\ttrain loss 0.8958\ttrain acc 0.0524\n",
            "[18/30 epochs][180/185 steps]\tlr 0.0004\ttrain loss 0.7425\ttrain acc 0.0524\n",
            "====================================================================================================\n",
            "[19/30 epochs][0/185 steps]\tlr 0.0004\ttrain loss 0.8299\ttrain acc 0.0524\n",
            "[19/30 epochs][10/185 steps]\tlr 0.0004\ttrain loss 0.7480\ttrain acc 0.0524\n",
            "[19/30 epochs][20/185 steps]\tlr 0.0004\ttrain loss 0.8772\ttrain acc 0.0524\n",
            "[19/30 epochs][30/185 steps]\tlr 0.0004\ttrain loss 0.7903\ttrain acc 0.0524\n",
            "[19/30 epochs][40/185 steps]\tlr 0.0004\ttrain loss 0.7639\ttrain acc 0.0524\n",
            "[19/30 epochs][50/185 steps]\tlr 0.0004\ttrain loss 0.8482\ttrain acc 0.0525\n",
            "[19/30 epochs][60/185 steps]\tlr 0.0004\ttrain loss 0.9517\ttrain acc 0.0525\n",
            "[19/30 epochs][70/185 steps]\tlr 0.0004\ttrain loss 0.8552\ttrain acc 0.0524\n",
            "[19/30 epochs][80/185 steps]\tlr 0.0004\ttrain loss 0.8257\ttrain acc 0.0524\n",
            "[19/30 epochs][90/185 steps]\tlr 0.0005\ttrain loss 0.8167\ttrain acc 0.0524\n",
            "[19/30 epochs][100/185 steps]\tlr 0.0005\ttrain loss 0.8918\ttrain acc 0.0524\n",
            "[19/30 epochs][110/185 steps]\tlr 0.0005\ttrain loss 0.7875\ttrain acc 0.0524\n",
            "[19/30 epochs][120/185 steps]\tlr 0.0005\ttrain loss 0.9163\ttrain acc 0.0524\n",
            "[19/30 epochs][130/185 steps]\tlr 0.0005\ttrain loss 0.8533\ttrain acc 0.0524\n",
            "[19/30 epochs][140/185 steps]\tlr 0.0005\ttrain loss 0.7740\ttrain acc 0.0524\n",
            "[19/30 epochs][150/185 steps]\tlr 0.0005\ttrain loss 0.9488\ttrain acc 0.0524\n",
            "[19/30 epochs][160/185 steps]\tlr 0.0005\ttrain loss 0.8626\ttrain acc 0.0525\n",
            "[19/30 epochs][170/185 steps]\tlr 0.0005\ttrain loss 0.8727\ttrain acc 0.0525\n",
            "[19/30 epochs][180/185 steps]\tlr 0.0005\ttrain loss 0.8252\ttrain acc 0.0525\n",
            "====================================================================================================\n",
            "[20/30 epochs][0/185 steps]\tlr 0.0005\ttrain loss 0.7763\ttrain acc 0.0525\n",
            "[20/30 epochs][10/185 steps]\tlr 0.0005\ttrain loss 0.7971\ttrain acc 0.0525\n",
            "[20/30 epochs][20/185 steps]\tlr 0.0005\ttrain loss 0.7631\ttrain acc 0.0525\n",
            "[20/30 epochs][30/185 steps]\tlr 0.0005\ttrain loss 0.8430\ttrain acc 0.0525\n",
            "[20/30 epochs][40/185 steps]\tlr 0.0005\ttrain loss 0.7411\ttrain acc 0.0525\n",
            "[20/30 epochs][50/185 steps]\tlr 0.0005\ttrain loss 0.7284\ttrain acc 0.0525\n",
            "[20/30 epochs][60/185 steps]\tlr 0.0005\ttrain loss 0.7775\ttrain acc 0.0525\n",
            "[20/30 epochs][70/185 steps]\tlr 0.0005\ttrain loss 0.9603\ttrain acc 0.0525\n",
            "[20/30 epochs][80/185 steps]\tlr 0.0005\ttrain loss 0.7245\ttrain acc 0.0525\n",
            "[20/30 epochs][90/185 steps]\tlr 0.0005\ttrain loss 0.8063\ttrain acc 0.0525\n",
            "[20/30 epochs][100/185 steps]\tlr 0.0005\ttrain loss 0.7390\ttrain acc 0.0525\n",
            "[20/30 epochs][110/185 steps]\tlr 0.0005\ttrain loss 0.7849\ttrain acc 0.0525\n",
            "[20/30 epochs][120/185 steps]\tlr 0.0005\ttrain loss 0.8805\ttrain acc 0.0525\n",
            "[20/30 epochs][130/185 steps]\tlr 0.0005\ttrain loss 0.9089\ttrain acc 0.0525\n",
            "[20/30 epochs][140/185 steps]\tlr 0.0005\ttrain loss 0.9422\ttrain acc 0.0525\n",
            "[20/30 epochs][150/185 steps]\tlr 0.0005\ttrain loss 0.9374\ttrain acc 0.0525\n",
            "[20/30 epochs][160/185 steps]\tlr 0.0005\ttrain loss 1.0819\ttrain acc 0.0525\n",
            "[20/30 epochs][170/185 steps]\tlr 0.0005\ttrain loss 0.9260\ttrain acc 0.0525\n",
            "[20/30 epochs][180/185 steps]\tlr 0.0005\ttrain loss 1.1550\ttrain acc 0.0525\n",
            "====================================================================================================\n",
            "[21/30 epochs][0/185 steps]\tlr 0.0005\ttrain loss 1.1325\ttrain acc 0.0525\n",
            "[21/30 epochs][10/185 steps]\tlr 0.0005\ttrain loss 0.8514\ttrain acc 0.0525\n",
            "[21/30 epochs][20/185 steps]\tlr 0.0005\ttrain loss 0.9815\ttrain acc 0.0525\n",
            "[21/30 epochs][30/185 steps]\tlr 0.0005\ttrain loss 0.9415\ttrain acc 0.0525\n",
            "[21/30 epochs][40/185 steps]\tlr 0.0005\ttrain loss 0.9366\ttrain acc 0.0524\n",
            "[21/30 epochs][50/185 steps]\tlr 0.0005\ttrain loss 0.9518\ttrain acc 0.0524\n",
            "[21/30 epochs][60/185 steps]\tlr 0.0005\ttrain loss 0.9623\ttrain acc 0.0524\n",
            "[21/30 epochs][70/185 steps]\tlr 0.0005\ttrain loss 0.7159\ttrain acc 0.0525\n",
            "[21/30 epochs][80/185 steps]\tlr 0.0005\ttrain loss 0.8580\ttrain acc 0.0525\n",
            "[21/30 epochs][90/185 steps]\tlr 0.0005\ttrain loss 0.7543\ttrain acc 0.0525\n",
            "[21/30 epochs][100/185 steps]\tlr 0.0005\ttrain loss 0.9146\ttrain acc 0.0525\n",
            "[21/30 epochs][110/185 steps]\tlr 0.0005\ttrain loss 0.8777\ttrain acc 0.0525\n",
            "[21/30 epochs][120/185 steps]\tlr 0.0005\ttrain loss 0.8731\ttrain acc 0.0525\n",
            "[21/30 epochs][130/185 steps]\tlr 0.0005\ttrain loss 0.9926\ttrain acc 0.0525\n",
            "[21/30 epochs][140/185 steps]\tlr 0.0005\ttrain loss 0.9423\ttrain acc 0.0525\n",
            "[21/30 epochs][150/185 steps]\tlr 0.0005\ttrain loss 0.8218\ttrain acc 0.0525\n",
            "[21/30 epochs][160/185 steps]\tlr 0.0005\ttrain loss 0.7836\ttrain acc 0.0525\n",
            "[21/30 epochs][170/185 steps]\tlr 0.0005\ttrain loss 0.9336\ttrain acc 0.0525\n",
            "[21/30 epochs][180/185 steps]\tlr 0.0005\ttrain loss 0.9749\ttrain acc 0.0525\n",
            "====================================================================================================\n",
            "[22/30 epochs][0/185 steps]\tlr 0.0005\ttrain loss 0.8721\ttrain acc 0.0525\n",
            "[22/30 epochs][10/185 steps]\tlr 0.0005\ttrain loss 0.7665\ttrain acc 0.0525\n",
            "[22/30 epochs][20/185 steps]\tlr 0.0005\ttrain loss 0.8216\ttrain acc 0.0525\n",
            "[22/30 epochs][30/185 steps]\tlr 0.0005\ttrain loss 0.9189\ttrain acc 0.0525\n",
            "[22/30 epochs][40/185 steps]\tlr 0.0005\ttrain loss 0.9579\ttrain acc 0.0525\n",
            "[22/30 epochs][50/185 steps]\tlr 0.0005\ttrain loss 0.8405\ttrain acc 0.0525\n",
            "[22/30 epochs][60/185 steps]\tlr 0.0005\ttrain loss 0.7267\ttrain acc 0.0525\n",
            "[22/30 epochs][70/185 steps]\tlr 0.0005\ttrain loss 0.8569\ttrain acc 0.0525\n",
            "[22/30 epochs][80/185 steps]\tlr 0.0005\ttrain loss 0.8582\ttrain acc 0.0525\n",
            "[22/30 epochs][90/185 steps]\tlr 0.0005\ttrain loss 0.8243\ttrain acc 0.0525\n",
            "[22/30 epochs][100/185 steps]\tlr 0.0005\ttrain loss 0.7899\ttrain acc 0.0525\n",
            "[22/30 epochs][110/185 steps]\tlr 0.0005\ttrain loss 0.8398\ttrain acc 0.0525\n",
            "[22/30 epochs][120/185 steps]\tlr 0.0005\ttrain loss 0.8241\ttrain acc 0.0525\n",
            "[22/30 epochs][130/185 steps]\tlr 0.0005\ttrain loss 0.8823\ttrain acc 0.0525\n",
            "[22/30 epochs][140/185 steps]\tlr 0.0005\ttrain loss 1.0808\ttrain acc 0.0525\n",
            "[22/30 epochs][150/185 steps]\tlr 0.0005\ttrain loss 0.9295\ttrain acc 0.0525\n",
            "[22/30 epochs][160/185 steps]\tlr 0.0005\ttrain loss 0.9724\ttrain acc 0.0525\n",
            "[22/30 epochs][170/185 steps]\tlr 0.0005\ttrain loss 0.9052\ttrain acc 0.0525\n",
            "[22/30 epochs][180/185 steps]\tlr 0.0005\ttrain loss 0.9116\ttrain acc 0.0525\n",
            "====================================================================================================\n",
            "[23/30 epochs][0/185 steps]\tlr 0.0005\ttrain loss 0.8172\ttrain acc 0.0525\n",
            "[23/30 epochs][10/185 steps]\tlr 0.0005\ttrain loss 0.8516\ttrain acc 0.0525\n",
            "[23/30 epochs][20/185 steps]\tlr 0.0005\ttrain loss 0.8975\ttrain acc 0.0525\n",
            "[23/30 epochs][30/185 steps]\tlr 0.0005\ttrain loss 0.8399\ttrain acc 0.0525\n",
            "[23/30 epochs][40/185 steps]\tlr 0.0005\ttrain loss 0.8515\ttrain acc 0.0525\n",
            "[23/30 epochs][50/185 steps]\tlr 0.0005\ttrain loss 0.9220\ttrain acc 0.0525\n",
            "[23/30 epochs][60/185 steps]\tlr 0.0005\ttrain loss 0.9571\ttrain acc 0.0525\n",
            "[23/30 epochs][70/185 steps]\tlr 0.0005\ttrain loss 0.9501\ttrain acc 0.0525\n",
            "[23/30 epochs][80/185 steps]\tlr 0.0005\ttrain loss 0.8658\ttrain acc 0.0525\n",
            "[23/30 epochs][90/185 steps]\tlr 0.0005\ttrain loss 0.7987\ttrain acc 0.0525\n",
            "[23/30 epochs][100/185 steps]\tlr 0.0005\ttrain loss 0.8590\ttrain acc 0.0525\n",
            "[23/30 epochs][110/185 steps]\tlr 0.0005\ttrain loss 0.7781\ttrain acc 0.0525\n",
            "[23/30 epochs][120/185 steps]\tlr 0.0005\ttrain loss 0.7810\ttrain acc 0.0525\n",
            "[23/30 epochs][130/185 steps]\tlr 0.0005\ttrain loss 0.9460\ttrain acc 0.0525\n",
            "[23/30 epochs][140/185 steps]\tlr 0.0005\ttrain loss 1.0213\ttrain acc 0.0525\n",
            "[23/30 epochs][150/185 steps]\tlr 0.0006\ttrain loss 0.8609\ttrain acc 0.0525\n",
            "[23/30 epochs][160/185 steps]\tlr 0.0006\ttrain loss 0.8984\ttrain acc 0.0525\n",
            "[23/30 epochs][170/185 steps]\tlr 0.0006\ttrain loss 0.8418\ttrain acc 0.0525\n",
            "[23/30 epochs][180/185 steps]\tlr 0.0006\ttrain loss 0.8737\ttrain acc 0.0525\n",
            "====================================================================================================\n",
            "[24/30 epochs][0/185 steps]\tlr 0.0006\ttrain loss 0.8619\ttrain acc 0.0526\n",
            "[24/30 epochs][10/185 steps]\tlr 0.0006\ttrain loss 0.8178\ttrain acc 0.0526\n",
            "[24/30 epochs][20/185 steps]\tlr 0.0006\ttrain loss 0.8296\ttrain acc 0.0526\n",
            "[24/30 epochs][30/185 steps]\tlr 0.0006\ttrain loss 0.8349\ttrain acc 0.0526\n",
            "[24/30 epochs][40/185 steps]\tlr 0.0006\ttrain loss 0.7819\ttrain acc 0.0526\n",
            "[24/30 epochs][50/185 steps]\tlr 0.0006\ttrain loss 0.8196\ttrain acc 0.0526\n",
            "[24/30 epochs][60/185 steps]\tlr 0.0006\ttrain loss 0.7648\ttrain acc 0.0526\n",
            "[24/30 epochs][70/185 steps]\tlr 0.0006\ttrain loss 0.9124\ttrain acc 0.0526\n",
            "[24/30 epochs][80/185 steps]\tlr 0.0006\ttrain loss 0.8841\ttrain acc 0.0526\n",
            "[24/30 epochs][90/185 steps]\tlr 0.0006\ttrain loss 0.7746\ttrain acc 0.0526\n",
            "[24/30 epochs][100/185 steps]\tlr 0.0006\ttrain loss 0.8381\ttrain acc 0.0526\n",
            "[24/30 epochs][110/185 steps]\tlr 0.0006\ttrain loss 0.9000\ttrain acc 0.0526\n",
            "[24/30 epochs][120/185 steps]\tlr 0.0006\ttrain loss 0.8717\ttrain acc 0.0526\n",
            "[24/30 epochs][130/185 steps]\tlr 0.0006\ttrain loss 0.7648\ttrain acc 0.0526\n",
            "[24/30 epochs][140/185 steps]\tlr 0.0006\ttrain loss 0.8141\ttrain acc 0.0526\n",
            "[24/30 epochs][150/185 steps]\tlr 0.0006\ttrain loss 0.9145\ttrain acc 0.0526\n",
            "[24/30 epochs][160/185 steps]\tlr 0.0006\ttrain loss 0.8572\ttrain acc 0.0526\n",
            "[24/30 epochs][170/185 steps]\tlr 0.0006\ttrain loss 0.8274\ttrain acc 0.0527\n",
            "[24/30 epochs][180/185 steps]\tlr 0.0006\ttrain loss 0.8778\ttrain acc 0.0527\n",
            "====================================================================================================\n",
            "[25/30 epochs][0/185 steps]\tlr 0.0006\ttrain loss 0.9178\ttrain acc 0.0527\n",
            "[25/30 epochs][10/185 steps]\tlr 0.0006\ttrain loss 0.7280\ttrain acc 0.0527\n",
            "[25/30 epochs][20/185 steps]\tlr 0.0006\ttrain loss 0.8864\ttrain acc 0.0527\n",
            "[25/30 epochs][30/185 steps]\tlr 0.0006\ttrain loss 0.7764\ttrain acc 0.0527\n",
            "[25/30 epochs][40/185 steps]\tlr 0.0006\ttrain loss 0.8734\ttrain acc 0.0527\n",
            "[25/30 epochs][50/185 steps]\tlr 0.0006\ttrain loss 0.7868\ttrain acc 0.0527\n",
            "[25/30 epochs][60/185 steps]\tlr 0.0006\ttrain loss 0.7718\ttrain acc 0.0527\n",
            "[25/30 epochs][70/185 steps]\tlr 0.0006\ttrain loss 0.8855\ttrain acc 0.0527\n",
            "[25/30 epochs][80/185 steps]\tlr 0.0006\ttrain loss 0.7561\ttrain acc 0.0527\n",
            "[25/30 epochs][90/185 steps]\tlr 0.0006\ttrain loss 0.9398\ttrain acc 0.0527\n",
            "[25/30 epochs][100/185 steps]\tlr 0.0006\ttrain loss 0.9499\ttrain acc 0.0527\n",
            "[25/30 epochs][110/185 steps]\tlr 0.0006\ttrain loss 0.8012\ttrain acc 0.0527\n",
            "[25/30 epochs][120/185 steps]\tlr 0.0006\ttrain loss 0.7355\ttrain acc 0.0527\n",
            "[25/30 epochs][130/185 steps]\tlr 0.0006\ttrain loss 0.8925\ttrain acc 0.0527\n",
            "[25/30 epochs][140/185 steps]\tlr 0.0006\ttrain loss 0.7492\ttrain acc 0.0527\n",
            "[25/30 epochs][150/185 steps]\tlr 0.0006\ttrain loss 0.8496\ttrain acc 0.0527\n",
            "[25/30 epochs][160/185 steps]\tlr 0.0006\ttrain loss 0.7601\ttrain acc 0.0527\n",
            "[25/30 epochs][170/185 steps]\tlr 0.0006\ttrain loss 0.7715\ttrain acc 0.0527\n",
            "[25/30 epochs][180/185 steps]\tlr 0.0006\ttrain loss 0.9769\ttrain acc 0.0527\n",
            "====================================================================================================\n",
            "[26/30 epochs][0/185 steps]\tlr 0.0006\ttrain loss 0.9196\ttrain acc 0.0527\n",
            "[26/30 epochs][10/185 steps]\tlr 0.0006\ttrain loss 0.8470\ttrain acc 0.0527\n",
            "[26/30 epochs][20/185 steps]\tlr 0.0006\ttrain loss 0.7199\ttrain acc 0.0527\n",
            "[26/30 epochs][30/185 steps]\tlr 0.0006\ttrain loss 0.7410\ttrain acc 0.0527\n",
            "[26/30 epochs][40/185 steps]\tlr 0.0006\ttrain loss 0.7833\ttrain acc 0.0528\n",
            "[26/30 epochs][50/185 steps]\tlr 0.0006\ttrain loss 0.9127\ttrain acc 0.0528\n",
            "[26/30 epochs][60/185 steps]\tlr 0.0006\ttrain loss 0.7831\ttrain acc 0.0528\n",
            "[26/30 epochs][70/185 steps]\tlr 0.0006\ttrain loss 0.7590\ttrain acc 0.0528\n",
            "[26/30 epochs][80/185 steps]\tlr 0.0006\ttrain loss 0.7293\ttrain acc 0.0528\n",
            "[26/30 epochs][90/185 steps]\tlr 0.0006\ttrain loss 0.8590\ttrain acc 0.0528\n",
            "[26/30 epochs][100/185 steps]\tlr 0.0006\ttrain loss 0.8087\ttrain acc 0.0528\n",
            "[26/30 epochs][110/185 steps]\tlr 0.0006\ttrain loss 0.8123\ttrain acc 0.0528\n",
            "[26/30 epochs][120/185 steps]\tlr 0.0006\ttrain loss 0.8005\ttrain acc 0.0528\n",
            "[26/30 epochs][130/185 steps]\tlr 0.0006\ttrain loss 0.8730\ttrain acc 0.0528\n",
            "[26/30 epochs][140/185 steps]\tlr 0.0006\ttrain loss 0.8614\ttrain acc 0.0528\n",
            "[26/30 epochs][150/185 steps]\tlr 0.0006\ttrain loss 0.7345\ttrain acc 0.0528\n",
            "[26/30 epochs][160/185 steps]\tlr 0.0006\ttrain loss 0.8324\ttrain acc 0.0528\n",
            "[26/30 epochs][170/185 steps]\tlr 0.0006\ttrain loss 0.7995\ttrain acc 0.0528\n",
            "[26/30 epochs][180/185 steps]\tlr 0.0006\ttrain loss 0.8522\ttrain acc 0.0528\n",
            "====================================================================================================\n",
            "[27/30 epochs][0/185 steps]\tlr 0.0006\ttrain loss 0.7700\ttrain acc 0.0528\n",
            "[27/30 epochs][10/185 steps]\tlr 0.0006\ttrain loss 0.7850\ttrain acc 0.0529\n",
            "[27/30 epochs][20/185 steps]\tlr 0.0006\ttrain loss 0.8671\ttrain acc 0.0529\n",
            "[27/30 epochs][30/185 steps]\tlr 0.0006\ttrain loss 0.8971\ttrain acc 0.0529\n",
            "[27/30 epochs][40/185 steps]\tlr 0.0006\ttrain loss 0.8238\ttrain acc 0.0529\n",
            "[27/30 epochs][50/185 steps]\tlr 0.0006\ttrain loss 0.7131\ttrain acc 0.0529\n",
            "[27/30 epochs][60/185 steps]\tlr 0.0006\ttrain loss 0.8725\ttrain acc 0.0529\n",
            "[27/30 epochs][70/185 steps]\tlr 0.0006\ttrain loss 0.8451\ttrain acc 0.0529\n",
            "[27/30 epochs][80/185 steps]\tlr 0.0006\ttrain loss 0.8853\ttrain acc 0.0529\n",
            "[27/30 epochs][90/185 steps]\tlr 0.0006\ttrain loss 0.7365\ttrain acc 0.0529\n",
            "[27/30 epochs][100/185 steps]\tlr 0.0006\ttrain loss 0.6164\ttrain acc 0.0529\n",
            "[27/30 epochs][110/185 steps]\tlr 0.0006\ttrain loss 0.9155\ttrain acc 0.0529\n",
            "[27/30 epochs][120/185 steps]\tlr 0.0006\ttrain loss 0.7593\ttrain acc 0.0529\n",
            "[27/30 epochs][130/185 steps]\tlr 0.0006\ttrain loss 0.8636\ttrain acc 0.0529\n",
            "[27/30 epochs][140/185 steps]\tlr 0.0006\ttrain loss 0.7619\ttrain acc 0.0529\n",
            "[27/30 epochs][150/185 steps]\tlr 0.0006\ttrain loss 0.8016\ttrain acc 0.0529\n",
            "[27/30 epochs][160/185 steps]\tlr 0.0006\ttrain loss 0.7623\ttrain acc 0.0529\n",
            "[27/30 epochs][170/185 steps]\tlr 0.0006\ttrain loss 0.8562\ttrain acc 0.0530\n",
            "[27/30 epochs][180/185 steps]\tlr 0.0006\ttrain loss 0.8091\ttrain acc 0.0530\n",
            "====================================================================================================\n",
            "[28/30 epochs][0/185 steps]\tlr 0.0006\ttrain loss 0.7187\ttrain acc 0.0530\n",
            "[28/30 epochs][10/185 steps]\tlr 0.0006\ttrain loss 0.7784\ttrain acc 0.0530\n",
            "[28/30 epochs][20/185 steps]\tlr 0.0006\ttrain loss 0.7344\ttrain acc 0.0530\n",
            "[28/30 epochs][30/185 steps]\tlr 0.0006\ttrain loss 0.7941\ttrain acc 0.0530\n",
            "[28/30 epochs][40/185 steps]\tlr 0.0006\ttrain loss 0.8280\ttrain acc 0.0530\n",
            "[28/30 epochs][50/185 steps]\tlr 0.0006\ttrain loss 0.7803\ttrain acc 0.0530\n",
            "[28/30 epochs][60/185 steps]\tlr 0.0006\ttrain loss 0.8284\ttrain acc 0.0530\n",
            "[28/30 epochs][70/185 steps]\tlr 0.0006\ttrain loss 0.9145\ttrain acc 0.0530\n",
            "[28/30 epochs][80/185 steps]\tlr 0.0006\ttrain loss 0.8605\ttrain acc 0.0530\n",
            "[28/30 epochs][90/185 steps]\tlr 0.0006\ttrain loss 0.9210\ttrain acc 0.0530\n",
            "[28/30 epochs][100/185 steps]\tlr 0.0006\ttrain loss 0.7682\ttrain acc 0.0531\n",
            "[28/30 epochs][110/185 steps]\tlr 0.0006\ttrain loss 0.8100\ttrain acc 0.0531\n",
            "[28/30 epochs][120/185 steps]\tlr 0.0006\ttrain loss 0.8059\ttrain acc 0.0531\n",
            "[28/30 epochs][130/185 steps]\tlr 0.0006\ttrain loss 0.8204\ttrain acc 0.0531\n",
            "[28/30 epochs][140/185 steps]\tlr 0.0006\ttrain loss 0.8400\ttrain acc 0.0531\n",
            "[28/30 epochs][150/185 steps]\tlr 0.0006\ttrain loss 0.8179\ttrain acc 0.0531\n",
            "[28/30 epochs][160/185 steps]\tlr 0.0006\ttrain loss 0.8252\ttrain acc 0.0531\n",
            "[28/30 epochs][170/185 steps]\tlr 0.0006\ttrain loss 0.8501\ttrain acc 0.0531\n",
            "[28/30 epochs][180/185 steps]\tlr 0.0006\ttrain loss 0.7317\ttrain acc 0.0531\n",
            "====================================================================================================\n",
            "[29/30 epochs][0/185 steps]\tlr 0.0006\ttrain loss 0.7953\ttrain acc 0.0531\n",
            "[29/30 epochs][10/185 steps]\tlr 0.0006\ttrain loss 0.7332\ttrain acc 0.0532\n",
            "[29/30 epochs][20/185 steps]\tlr 0.0006\ttrain loss 0.8572\ttrain acc 0.0532\n",
            "[29/30 epochs][30/185 steps]\tlr 0.0006\ttrain loss 0.7809\ttrain acc 0.0532\n",
            "[29/30 epochs][40/185 steps]\tlr 0.0006\ttrain loss 0.7878\ttrain acc 0.0532\n",
            "[29/30 epochs][50/185 steps]\tlr 0.0006\ttrain loss 0.7774\ttrain acc 0.0532\n",
            "[29/30 epochs][60/185 steps]\tlr 0.0006\ttrain loss 0.7832\ttrain acc 0.0532\n",
            "[29/30 epochs][70/185 steps]\tlr 0.0006\ttrain loss 0.7511\ttrain acc 0.0532\n",
            "[29/30 epochs][80/185 steps]\tlr 0.0006\ttrain loss 0.8590\ttrain acc 0.0532\n",
            "[29/30 epochs][90/185 steps]\tlr 0.0006\ttrain loss 0.7212\ttrain acc 0.0533\n",
            "[29/30 epochs][100/185 steps]\tlr 0.0006\ttrain loss 0.7598\ttrain acc 0.0533\n",
            "[29/30 epochs][110/185 steps]\tlr 0.0006\ttrain loss 0.7753\ttrain acc 0.0533\n",
            "[29/30 epochs][120/185 steps]\tlr 0.0006\ttrain loss 0.8386\ttrain acc 0.0533\n",
            "[29/30 epochs][130/185 steps]\tlr 0.0006\ttrain loss 0.7395\ttrain acc 0.0533\n",
            "[29/30 epochs][140/185 steps]\tlr 0.0006\ttrain loss 0.7181\ttrain acc 0.0533\n",
            "[29/30 epochs][150/185 steps]\tlr 0.0006\ttrain loss 0.8404\ttrain acc 0.0533\n",
            "[29/30 epochs][160/185 steps]\tlr 0.0006\ttrain loss 0.7654\ttrain acc 0.0533\n",
            "[29/30 epochs][170/185 steps]\tlr 0.0006\ttrain loss 0.7660\ttrain acc 0.0534\n",
            "[29/30 epochs][180/185 steps]\tlr 0.0006\ttrain loss 0.7701\ttrain acc 0.0534\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R_jHggjiqIzz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}